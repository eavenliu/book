# 消息中间件-kafka

## 设计底层
Kafa的设计底层主要为了支撑和实现三个方面：
1. **发布/订阅**：读取和写入数据流，如消息传递系统。
2. **流处理**：编写可扩展的流处理应用程序，用以实时响应事件。
3. **存储**：将数据流安全地存储在一个分布式、可复制的、高容错的集群中。

![image](https://note.youdao.com/yws/res/17926/FF530E1C2DC946B0A8F3C3D5F45115E1)

综上，Kafka的最大生产用途是：**用于构建实时数据管道和流应用程序，它具有水平可扩展性，容错性，快速性。**

## 主体介绍（围绕核心抽象概念）

### Kafka是一个分布式的数据流平台，意味着什么？
一个分布式的数据流平台包含三个关键功能：
- 可以进行发布和订阅记录流，类似一个消息队列和企业消息传递系统；
- 以一种容错的方式持久化存储记录流；
- 几乎实时的处理记录流（在记录发生的时候）。

### Kafka通常用于两大类应用
- 构建实时数据流管道用以在系统或应用程序之间可靠的获取数据；
- 构建实时流处理应用程序用以转换或响应数据流。

### 那么Kafka是通过什么样的设计和实现来做成这些事的呢？

首先有几个概念：
- Kafka作为一个集群可运行在一个或多个可跨多个数据中心的服务器上（即程序运行无状态，并对集群友好）；
- Kafka集群是根据被称作“topics”的概念来对应存储数据记录流的；
- **每条数据记录都包含：一个key，一个value和一个时间戳**。

Kafka有四大核心API：
- Producer API ：生产者API，允许应用程序发布记录流到一个或多个Kafka的Topic；
- Consumer API ：消费者API，允许应用程序订阅一个或多个Topic并处理对应Topic中生成的记录流；
- Streams API ：允许应用程序充当流处理器，消费来自一个或多个Topic的输入流并生成一个或多个输出Topic的输出流，从而有效地将输入流转换为输出流；
- Connector API ：允许构建和运行将Kafka Topics连接到现有应用程序或数据系统的可重用生产者或消费者。 例如，关系数据库的连接器可能捕获对表的每个更改。

![image](https://note.youdao.com/yws/res/18019/0C9C8B1FFE884C2BB765B47A3889DAAD)

在Kafka中，客户端和服务器之间的通信是通过简单，高性能，语言无关的TCP协议完成的。 我们在后面会专门分析Kakfa中的协议定义和相关使用。

### Topics and Logs

让我们首先深入探讨Kafka为记录流提供的核心抽象- **Topic**。

Topic是对应发布的记录的类别或订阅源名称。 Kafka中的Topic总是对应多个订阅者; 也就是说，一个Topic可以有零个，一个或多个消费者订阅写入它的数据。

**对于每一个topic，Kafka集群都维护一个分区日志**，分区日志的数据结构如下图：

![image](https://note.youdao.com/yws/res/18023/48DA3D4035A945A8B3509BC3981D91E8)

每个分区都是一个有序的，不可变的记录序列，不断附加到结构化的提交日志中。 **分区中的记录每个都被分配一个称为偏移的顺序ID号，它唯一地标识分区中的每个记录**。

**Kafka集群持久地保留所有已发布的记录 - 无论它们是否已被消费 - 使用可配置的保留期。 例如，如果保留策略设置为两天，则在发布记录后的两天内，它可供使用，之后将被丢弃以释放空间。 Kafka的性能在数据大小方面实际上是恒定的，因此长时间存储数据不是问题**。（为什么性能在数据大小方面实际上的是恒定的，后续持续分析）

![image](https://note.youdao.com/yws/res/18045/6FF349ECD75A444EB004EF1D974D8535)

事实上，**每个消费者都保留有唯一的元数据来记录该消费者在日志中的偏移或位置**。 这种偏移由消费者控制：通常消费者在读取记录时会线性地提高其偏移量，但事实上，由于消费者控制位置，它可以按照自己喜欢的任何顺序消费记录。 例如，消费者可以重置为较旧的偏移量以重新处理过去的数据，或者跳到最近的记录并从“现在”开始消费。

这些功能组合意味着Kafka消费者非常独立 - 他们可以随意的消费或退出，但对集群或其他消费者没有太大影响。例如，您可以使用我们的命令行工具“tail”任何主题的内容，而无需更改任何现有使用者所消耗的内容。

**日志中的分区有多种用途**：
- 首先，它们允许日志扩展并超出合适单个服务器的大小。每个单独的分区必须适合托管它的服务器，但主题可能有许多分区，因此它可以处理任意数量的数据。 
- 其次，它们充当了并行性的单位 -- 更多的是在一点上。

### 分配（distribution）
**日志的分区分布在Kafka集群中的服务器上，每个服务器处理数据并请求分区的共享。每个分区都在可配置数量的服务器上进行复制，以实现容错**。

**每个分区都有一个服务器充当“leader”，零个或多个服务器充当“fllowers”**。 leader处理分区的所有读取和写入请求，而fllowers被动地复制领导者。 如果leader所在服务器脱离集群，其中一个fllower将自动成为新的领导者。 **每个服务器都充当其某些分区的leader和其他服务器的fllower，因此负载在群集中很平衡**。

### 地域复制（Geo-Replication）
Kafka MirrorMaker为kafaka集群提供地域复制支持。使用MirrorMaker，消息可以被跨多个数据中心或云区域复制。 你可以在主动/被动方案中使用它进行备份和恢复; 或者在主动/主动方案中，使数据更接近用户，或支持数据位置要求。

### 生产者/发布（Producers）
生产者将数据发布到它们指定的topics中。同时生产者负责选择将记录分配给Topic中某个分区。这可以通过循环方式完成，只是为了分区的负载均衡，或者可以根据一些语义分区功能（例如基于记录中的某个key）来完成。 

### 消费者/订阅（Consumers）
**消费者使用消费者组名称标记自己，并且发布到topic的每个记录被传递到每个订阅消费者组中的一个消费者实例**。消费者实例可以在互相独立的进程中，也可以在不同的机器中。
- **如果所有消费者实例存在于一个相同的消费者组（consumer group），则一个记录只会传递给一个消费者实例，有效地在消费者实例上进行负载平衡。**
- **如果所有消费者实例存在于多个不同的消费者组（consumer group），则每个记录将广播到所有消费者进程。**

如下图所示，两台服务器组成的kafka集群托管了4个分区：P0-P3。有两个消费者组：cgA有2个消费者实例、cgB有4个消费者实例

![image](https://note.youdao.com/yws/res/18125/BD311C9E1F07498DB202E2C8586DB21B)

然而，更常见的是，我们发现topic具有少量的消费者组，每个“logic subscriber”一个。 每个组由许多用于可伸缩性和容错的消费者实例组成。这只不过是发布-订阅语义，其中订阅者是消费者组而不是单个进程。

**在Kafka中实现消费的方式是：通过给消费者组中的每个消费者实例划分日志中的分区，以便每个实例在任何时间点都是作为分配的“公平份额”的独占消费者。维护组中成员资格的过程由Kafka协议动态处理。 如果新实例加入该组，他们将从该组的其他成员接管一些分区; 如果实例死亡，其分区将分发给其余实例**。

Kafka仅提供单个分区内记录的总排序，而不是topic中不同分区之间的记录。对于大多数应用程序而言，按分区排序与按键分区数据的能力相结合就足够了。 但是，如果您需要对记录进行总排序，那么可以使用仅包含一个分区的topic来实现，但这将意味着每个消费者组只有一个消费者进程。

### 多租户（Multi-tenancy）
您可以将Kafka部署为多租户解决方案。 通过配置哪些topics可以生成或消费数据来启用多租户。对应的配额也有运营支持。管理员可以定义和强制执行配额，以控制客户端使用的代理资源。 有关更多信息，请参阅[安全文档](https://kafka.apache.org/documentation/#security)。

### 担保（Guarantees）
在高级别Kafka提供以下担保：
- 生产者发送到特定topic分区的消息将按其发送的顺序附加到日志中。也就是说，如果记录M1由与记录M2相同的生产者发送，并且首先发送M1，则M1将具有比M2更低的偏移并且在日志中更早出现。
- 消费者实例查看记录是按照它们存储在日志中的顺序。
- 对于具有复制因子N（N个服务器组成的集群）的topic，我们将容忍最多N-1个服务器故障，而不会丢失任何提交到日志的记录。

更多担保相关的详情后面继续分析

### Kafka作为消息系统
Kafka的流概念与传统的企业消息传递系统相比如何？

#### 模型优势
**消息传统上有两种模型：队列和发布-订阅**。
- 在队列模型中，消费者池可以从服务器读记录并且将记录转到其中一个消费者中处理; 
- 在发布-订阅模型中，记录被广播给所有消费者。

这两种模型中的每一种都有优点和缺点：
- 队列的优势在于它允许你在多个消费者实例上划分数据处理，从而可以扩展你的处理性能。不幸的是，一旦一个进程读取它已经消失的数据，队列就不是多用户。 
- 发布-订阅允许你将数据广播到多个进程，但由于每条消息都发送给每个订阅者，因此无法进行记录扩展处理。

**Kafka的消费者组概念囊括了上面的两种模型**：与队列一样，消费者组允许您将处理划分为一组进程（消费者组的成员）； 与发布-订阅一样，Kafka允许你向多个消费者组广播消息。

Kafka模型的优势在于每个Topic都具有这些属性：它可以扩展处理并且也是多订阅--不需要只选择其中一个模型。
#### 顺序保证和并行优势
**与传统的消息系统相比，Kafka具有更强的顺序性保证**。

传统队列在服务器上按顺序保留记录，如果多个消费者从队列中消耗，则服务器按照存储顺序分发记录。 但是，虽然服务器按顺序分发记录，但是记录是异步传递给消费者的，因此它们可能会在不同的消费者上无序传送。 这实际上意味着在存在并行消耗的情况下丢失记录的顺序。 消息传递系统通常通过具有“独占消费者”的概念来解决这个问题，该概念只允许一个进程从队列中消耗，但这当然意味着处理中没有并行性。

Kakfa在保证顺序上做得更好。**通过在topic中具有并行性概念：分区，Kafka能够在消费者进程池中提供顺序保证和负载平衡**。

这是通过将topic中的分区分配给消费者者组中的消费者来实现的，以便每个分区仅由该组中的一个消费者使用。通过这样做，我们确保消费者是该分区的唯一读者并按顺序使用数据。 由于有许多分区，这仍然可以平衡许多消费者实例的负载。 但请**注意，消费者组中的消费者实例不能超过分区**。

### Kafka作为存储系统
**任何允许消息发布与消费分离的消息队列实际上充当了正在进行的消息的存储系统**。Kafka的不同之处在于它是一个非常好的存储系统。

**写入Kafka的数据将写入磁盘并进行复制以保证容错性。 Kafka允许生产者等待确认，以便在完全复制之前写入不被认为是完整的，并且即使写入的服务器失败也保证写入仍然存在。**

Kafka对于数据写入磁盘也很友好 - 无论服务器上有50KB还是50TB的持久数据，Kafka都会执行相同的操作。

由于认真对待存储并允许客户端控制其读取位置，**你可以将Kafka视为一种专用于高性能、低延迟提交日志存储、复制和传播的分布式文件系统**。

有关Kafka的提交日志存储和复制设计的详细信息，请阅读此页面：https://kafka.apache.org/documentation/#design。

### Kafka用于流处理
在Kafka中，**流处理器（stream processor）是指从输入topic获取连续数据流，对此输入流执行某些处理后生成连续数据流到输出topic**。（例如，一个零售应用程序可能会接收销售和发货的数据输入流，并输出重新排序流和根据此数据计算的价格调整结果。）

可以使用生产者和消费者API直接进行简单处理。然而，对于更复杂的转换，Kafka提供了完全集成的Streams API。Stream API允许构建执行复杂的流处理的应用程序，这些应用程序可以针对流进行聚合或将流连接在一起。

此工具有助于解决实时流处理应用程序面临的难题：处理无序数据，在代码更改时重新处理输入，执行有状态计算等。

Stream API构建在Kafka提供的核心原语上：**它使用生产者和消费者API进行输入，使用Kafka进行有状态存储，并在流处理器实例之间使用相同的组机制来实现容错**。

### 将流碎片组合
消息传递，存储和流处理的这种组合可能看起来很不寻常，但它对于Kafka作为流媒体平台的作用至关重要。

像HDFS这样的分布式文件系统允许存储静态文件以进行批处理，这样的系统能有效的存储和处理过去的历史数据。

传统的企业消息传递系统允许处理订阅后到达的未来消息。以这种方式构建的应用程序在未来数据到达时处理。

**Kafka结合了这两种功能，这种组合对于Kafka作为流处理应用程序平台以及流数据管道至关重要**。

**通过组合存储和低延迟订阅，流应用程序可以以相同的方式处理过去和未来的数据**。也就是说，单个应用程序可以处理历史存储的数据，而不是被动等待未来数据的到来，它可以在未来数据到达时继续处理。**这是包含批处理以及消息驱动应用程序的流处理的一般概念。**

同样，对于流数据流水线，订阅实时事件的组合使得可以将Kafka用于极低延迟的流水线;但是，能够可靠地存储数据，使得也可将其用于必须保证数据传输安全的业务，或者与仅定期加载数据或可能长时间停机以进行维护的离线系统集成。流处理设施可以在数据到达时对其进行转换。

## Tutorials
我们在这里简单介绍一下关于Kafka的基础使用：
### 搭建Kafka服务器
#### 1、下载代码
通过https://www.apache.org/dyn/closer.cgi?path=/kafka/2.3.0/kafka_2.12-2.3.0.tgz下载对应的kafka_2.12-2.3.0.tgz代码包，然后进行解压：
```
> tar -xzf kafka_2.12-2.3.0.tgz
> cd kafka_2.12-2.3.0
```
#### 2、启动Kafka服务
配置对应的Zookeeper服务器（具体作用后面会详细分析）和一些基础配置，启动服务：
```
> bin/kafka-server-start.sh config/server.properties
[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)
[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)
...
```
#### 3、创建一个Topic
让我们创建一个名为“test”的Topic，它只包含一个分区，只有一个副本：

```
> bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test
```
然后查看Topic列表：

```
> bin/kafka-topics.sh --list --bootstrap-server localhost:9092
test
```
或者，可以将代理配置为在发布不存在的Topic时自动创建Topic，而不是手动创建Topic。

#### 4、向Topic中发送消息
Kafka附带一个命令行客户端，它将从文件或标准输入中获取输入，并将其作为消息发送到Kafka集群。 默认情况下，每行将作为单独的消息发送。

运行producer，然后在控制台中键入一些消息以发送到服务器。

```
> bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
This is a message
This is another message
```
#### 5、启动一个消费者进程
Kafka还有一个命令行消费者，它会将消息转储到标准输出。下面是启动一个消费者并从头开始读取Topic为“test”的消息
```
> bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
This is a message
This is another message
```
如果你在不同的终端中运行上述每个命令，那么您现在应该能够在生产者终端中键入消息并看到它们出现在消费者终端中。

所有命令行工具都有其他选项。

#### 6、开启集群
对于上述步骤，本质上来说，单个broker是一个大小为1的集群。那么我们现在来多启动两个broker实例，构建一个拥有3个节点的集群（这里是在本地构建）。

首先，我们为每个broker创建一个配置文件）：

```
> cp config/server.properties config/server-1.properties
> cp config/server.properties config/server-2.properties
```
现在编辑这些新文件并设置以下属性：

```
config/server-1.properties:
    broker.id=1
    listeners=PLAINTEXT://:9093
    log.dirs=/tmp/kafka-logs-1
 
config/server-2.properties:
    broker.id=2
    listeners=PLAINTEXT://:9094
    log.dirs=/tmp/kafka-logs-2
```
**broker.id属性**是群集中每个节点的唯一且永久的名称。 我们必须覆盖端口和日志目录，因为我们在同一台机器上运行这些，并且我们不希望让所有brokers尝试在同一端口上注册或覆盖彼此的数据。

现在启动这两个新增的节点：

```
> bin/kafka-server-start.sh config/server-1.properties &
...
> bin/kafka-server-start.sh config/server-2.properties &
...
```
现在创建一个复制因子为3，单个分区的新Topic：

```
> bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 3 --partitions 1 --topic my-replicated-topic
```
好的，但现在我们有一个集群，我们怎么知道哪个broker正在做什么？ 要查看运行“describe topics”命令：
```
> bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic my-replicated-topic
Topic:my-replicated-topic   PartitionCount:1    ReplicationFactor:3 Configs:
    Topic: my-replicated-topic  Partition: 0    Leader: 1   Replicas: 1,2,0 Isr: 1,2,0

```
这是输出的解释。第一行给出了所有分区的摘要，每个附加行提供有关一个分区的信息。 由于此主题只有一个分区，因此只有一行。
- “leader”是负责给定分区的所有读写的节点。每个节点将成为随机选择的分区部分的领导者。
- “replicas”是复制此分区日志的节点列表，无论它们是否为领导者，或者即使它们当前处于活动状态。
- “isr”是“同步”复制品的集合。这是副本列表的子集，该列表当前处于活跃状态并且已经被领导者捕获。

请注意，在我的示例中，节点1是Topic的唯一分区的领导者。

我们可以在我们创建的原始主题-“test”上运行相同的命令，以查看它的位置：

```
> bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test
Topic:test  PartitionCount:1    ReplicationFactor:1 Configs:
    Topic: test Partition: 0    Leader: 0   Replicas: 0 Isr: 0
```

让我们向我们的新主题发布一些消息：

```
> bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic
...
my test message 1
my test message 2
```
然后来消费这些消息：

```
> bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic
...
my test message 1
my test message 2
```
现在让我们测试一下集群的容错性。broker1充当领导者所以让我们杀了它：

```
> ps aux | grep server-1.properties
7564 ttys002    0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.8/Home/bin/java...
> kill -9 7564
```
领导已切换到其中一个follower，节点1不再处于同步副本集中：

```
> bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic my-replicated-topic
Topic:my-replicated-topic   PartitionCount:1    ReplicationFactor:3 Configs:
    Topic: my-replicated-topic  Partition: 0    Leader: 2   Replicas: 1,2,0 Isr: 2,0
```
但即使最初接受写入的领导者已经失败，这些消息仍可供消费：

```
> bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic
...
my test message 1
my test message 2
```
#### 7、使用Kafka Connect导入/导出数据
从控制台写入数据并将其写回控制台是一个方便的起点，但您可能希望使用其他来源的数据或将数据从Kafka导出到其他系统。 对于许多系统，您可以使用Kafka Connect导入或导出数据，而不是编写自定义集成代码。

Kafka Connect是Kafka附带的工具，可以向Kafka导入和导出数据。 它是一个可扩展的工具，可以运行连接器，实现与外部系统交互的自定义逻辑。 在本快速入门中，我们将了解如何使用简单的连接器运行Kafka Connect，这些连接器将数据从文件导入Kafka主题并将数据从Kafka主题导出到文件。

首先，我们将首先创建一些种子数据进行测试：

```
> echo -e "foo\nbar" > test.txt
```
接下来，我们将启动以独立模式运行的两个连接器，这意味着它们在单个本地专用进程中运行。 我们提供三个配置文件作为参数。 第一个始终是Kafka Connect流程的配置，包含常见配置，例如要连接的Kafka代理和数据的序列化格式。 其余配置文件均指定要创建的连接器。 这些文件包括唯一的连接器名称，要实例化的连接器类以及连接器所需的任何其他配置。

```
> bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties

```
Kafka附带的这些示例配置文件使用您之前启动的默认本地群集配置并创建两个连接器：第一个是源连接器，它从输入文件读取行并生成每个Kafka主题，第二个是宿连接器 从Kafka主题读取消息并将每个消息作为输出文件中的一行生成。

在启动过程中，您将看到许多日志消息，包括一些指示正在实例化连接器的日志消息。 一旦Kafka Connect进程启动，源连接器应该开始从test.txt读取行并生成主题connect-test，并且接收器连接器应该开始从主题connect-test读取消息并将它们写入文件测试.sink.txt。 我们可以通过检查输出文件的内容来验证数据是否已通过整个管道传递：

```
> more test.sink.txt
foo
bar
```
请注意，数据存储在Kafka主题connect-test中，因此我们还可以运行控制台消费者来查看主题中的数据（或使用自定义使用者代码来处理它）：

```
> bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning
{"schema":{"type":"string","optional":false},"payload":"foo"}
{"schema":{"type":"string","optional":false},"payload":"bar"}
...
```
连接器继续处理数据，因此我们可以将数据添加到文件中，并看到它在管道中移动：

```
> echo Another line>> test.txt
```
你应该看到该行出现在控制台消费者输出和接收器文件中。

#### 8、使用Kafka Streams处理数据
Kafka Streams是一个客户端库，用于构建任务关键型实时应用程序和微服务，其中输入和输出数据存储在Kafka集群中。 Kafka Streams结合了在客户端编写和部署标准Java和Scala应用程序的简单性以及Kafka服务器端集群技术的优势，使这些应用程序具有高度可扩展性，弹性，容错性，分布式等等。 

详细示例见：https://kafka.apache.org/23/documentation/streams/quickstart

## 用例
以下是ApacheKafka的一些常用用例的描述。 有关这些领域的概述，请参阅此[博客文章](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)。
### 消息
Kafka可以替代更传统的消息队列。消息代理（broker）被应用使用有多种原因（将处理与数据生成器分离（解耦系统），削峰限流，缓冲未处理的消息等）。 与大多数消息传递系统相比，Kafka具有更好的吞吐量，内置分区，复制和容错功能，这使其成为大规模消息处理应用程序的理想解决方案。
根据我们的经验，消息传递的使用通常相对较低，但可能需要较低的端到端延迟，并且通常取决于Kafka提供的强大的耐用性保证。

在这个领域，Kafka可与传统的消息传递系统（如ActiveMQ或RabbitMQ）相媲美。

### 网站行为追踪
Kafka的原始用例是能够将用户行为轨迹管道定义重建为一组实时发布 - 订阅源。 这意味着站点活动（页面查看，搜索或用户可能采取的其他操作）将发布到中心主题，每个活动类型包含一个主题。 这些订阅流可用于一系列业务场景，包括实时处理，实时监控以及加载到Hadoop或离线数据仓库系统以进行离线处理和报告。
活动跟踪通常非常高，因为为每个用户页面视图生成了许多活动消息。

### 度量（Metrics）
**Kafka通常用于运营监控数据**。这涉及从分布式应用程序中聚合统计信息以生成目标统计数据。

### 日志聚合
许多人使用Kafka作为日志聚合解决方案的替代品。日志聚合通常从服务器收集物理日志文件，并将它们放在中央位置（可能是文件服务器或HDFS）进行处理。 Kafka抽象出文件的细节，并将日志或事件数据更清晰地抽象为消息流。这允许更低延迟的处理并更容易支持多个数据源和分布式数据消耗。 **与Scribe或Flume等以日志为中心的系统相比，Kafka提供了同样出色的性能，由于复制而具有更强的耐用性保证，以及更低的端到端延迟**。

### 流处理(Stream processing)
许多Kafka用户在处理由多个阶段组成的管道时处理数据，其中原始输入数据从Kafka主题中消费，然后聚合，丰富或以其他方式转换为新主题以供进一步消费或后续处理。

例如，用于推荐新闻文章的处理管道可以从RSS订阅源抓取文章内容并将其发布到“文章”主题; 进一步处理可以对此内容进行规范化或重复数据删除，并将已清理的文章内容发布到新主题; 最终处理阶段可能会尝试向用户推荐此内容。此类处理管道基于各个主题创建实时数据流的图形。

从0.10.0.0开始，Apache Kafka中提供了一个名为Kafka Streams的轻量级但功能强大的流处理库，用于执行上述数据处理。除了Kafka Streams之外，其他开源流处理工具包括Apache Storm和Apache Samza，以及Apache Flink。

### 事件源(Event sourcing)
**事件源采用确保应用程序状态的所有更改都存储为一系列事件。我们不仅可以查询这些事件，还可以使用事件日志重建过去的状态，并作为自动调整状态以应对追溯更改的基础。**

事件源是一种应用程序设计风格，其中状态更改被记录为按时间排序的记录序列。 Kafka对非常大的存储日志数据的支持使其成为以这种风格构建的应用程序的出色后端。

事件源的详细思想：https://martinfowler.com/eaaDev/EventSourcing.html

### 提交日志（commit log）
**Kafka可以作为分布式系统的一种外部提交日志。该日志有助于在节点之间复制数据，并充当故障节点恢复其数据的重新同步机制**。Kafka中的日志压缩功能有助于支持此用法。 在这种用法中，Kafka类似于Apache BookKeeper项目。

## Specification
### 设计思想
#### 动机
**我们设计的Kafka需要能够成为一个统一的平台用以处理公司所有的实时数据流**。所以必须要考虑很多肯定的场景需要：
- 它必须**具有高吞吐量**以支持超量事件流，例如实时日志聚合；
- 它需要**优雅地处理大型数据日志**，以便能够支持离线系统的定期数据加载；
- 这也意味着系统**必须支撑消息的低延迟传递**，以兼容更传统的消息传递场景；
- 在高并发、实时反馈场景中，希望流平台**支持对这些Feed进行分区，分布式，实时处理，以创建新的派生Feed**。这启发了我们设计分区和消费者模式；
- 最后，在将流馈送到其他数据系统进行服务的情况下，我们知道系统必须能够在出现机器故障时**保证容错**。

支持这些用途使我们的设计具有许多独特的元素，更类似于数据库日志而不是传统的消息传递系统。我们将在以下部分概述设计的一些元素。

#### 底层建筑
##### 对硬盘数据存储的设计构建-文件系统
**Kafka在很大程度上依赖于文件系统来存储和缓存消息**。人们普遍认为“磁盘速度慢”，这让人们怀疑持久性结构能否提供有竞争力的性能。 **事实上，根据使用方式，磁盘速度比人们预期的要慢得多，速度也快得多; 并且设计合理的磁盘结构通常可以与网络一样快。**

关于磁盘性能的关键事实是硬盘的吞吐量与过去十年中磁盘搜索的延迟有所不同。 因此，具有六个7200rpm SATA RAID-5阵列的JBOD配置的线性写入性能大约为600MB /秒，但随机写入的性能仅为大约100k /秒 - 相差超过6000X。 **这些线性读取和写入是所有使用模式中最可预测的，并且由操作系统进行了大量优化**。 现代操作系统提供预读和后写技术，以大块多次预取数据，并将较小的逻辑写入分组为大型物理写入。 有关此问题的进一步讨论可以在此ACM Queue文章中找到; 他们实际上发现**顺序磁盘访问在某些情况下可能比随机内存访问更快**！

为了弥补这种性能上的差距, 现代操作系统, 更多使用内存来为磁盘做缓存. 现代的操作系统更乐意使用所有的空闲内存为磁盘做缓存, 在内存的回收上只需要花费极小的代价. 所有的磁盘读写都通过统一的缓存. 如果没有使用direct I/O这个开关, 这种特性不会很容易被屏蔽掉. 因此,即使一个进程内部独立维持一个数据缓存, 那么数据也有可能在系统页中再被缓存一次, 所有的数据都会被存储两次

此外，我们基于jvm上面构建应用，任何花时间使用Java内存的人都知道两件事：
1. **内存中存有大量的对象需要消耗很高，通常会使存储的数据大小加倍（或更糟）**。
2. **随着堆内数据的增加，Java垃圾收集变得越来越频繁和缓慢**。

考虑到这些因素, 使用文件系统并使用页缓存机制比自己去进行内存缓存或使用其他存储结构更为有效–我们访问内存的时候已经起码至少访问了两次缓存, 很有可能在写字节的时候也是两次存储而非单次. 这样做的话, 在一个缓存达到32GB的机器上, 可以减少GC的代价, 这样也可以减少代码在维护缓存和系统文件间的一致性, 比再尝试新的方法有更高的正确行. 如果你对磁盘的使用充分利用到线性读, 那么预取机制将会很有效的在每次磁盘读取时实现填充好缓存空间.

这意味设计非常简单, 系统不是更多把数据保存到内存空间, 在内存空间耗尽时才赶紧写入到文件系统中, 相反的, 所有的数据都被马上写入到文件系统的日志文件中, 但没有必要马上进行flush磁盘操作. 只是把数据传输到系统内核的页面空间中去了.

这种**以pagecache为中心的设计风格**在一篇关于[Varnish设计](http://varnish-cache.org/docs/trunk/phk/notes.html)的文章中有所描述。

##### 数据的访问性能--常量耗时需求
在消息系统中, 大部分持久化的数据结构通常使用一个消费者队列一个btree结构, 或其他随机读取的数据结构用于维持消息的元数据信息。

**BTrees是最通用的数据结构，可以在消息传递系统中支持各种事务和非事务语义**。但它们确实具有相当高的成本：Btree操作时间复杂度是O(logN)。通常认为O(logN)基本上等于常量时间，但对于磁盘操作则不然。磁盘搜索速度为10毫秒，每个磁盘一次只能进行一次搜索，所以并行性有限。因此，即使存在少数磁盘寻道也会导致非常高的开销。由于存储系统将非常快速的高速缓存操作与非常慢的物理磁盘操作混合在一起，因此随着数据在高速缓存的中的增加，**树结构的观察性能通常是超线性的---即：将数据加倍会使事情变得比两倍慢**。

直觉来看, **一个持久化队列可以使用简单的读和追加数据到文件的日志方式进行实现, 这种结构有一个好处是, 所有操作都是O(1)性能的, 而且读和写入数据不会相互阻塞, 这样性能和数据的大小完全无关**, 一台服务器可以完全充分利用了廉价, 低速的1+TB SATA 硬盘, 虽然它们的寻道性能不高, 但是他们以3分之一的价格和3倍的容量接受大量的读写请求

**在没有任何性能损失的情况下访问几乎无限的磁盘空间意味着我们可以提供消息传递系统中通常不具备的一些功能**。 例如，在Kafka中，我们可以在消息消费后立即删除消息，而不是试图将消息保留一段相对较长的时间（例如一周）。 正如我们将要描述的那样，这为消费者带来了极大的灵活性。

#### 效率（Efficiency）
我们在效率上投入了众多的努力, 一个我们的主要用例是具有大吞吐量的web活动日志, 每页面的每次访问都会产生好几十次的写, 进一步, 我们假定每次消息发布, 至少会被一个消费者读取(经常情况下是多个消费者), 因此, 我们努力使消费消息的代价尽可能小。

从构建一些相识的系统的经验中, 我们也发现, 有效的多租户操作是提升性能的关键。下游的基础服务很容易由于程序的很小的使用错误成为瓶颈, 例如, 一些小的变化很常导致一些新的问题, 我们可以非常快速在程序发布到基础平台前, 进行迭代测试, 这对需要在集中式的集群里跑几十个, 几千个应用时, 程序每天都在变动时非常有用。

前面我们讨论了磁盘的性能, 没有效率的磁盘访问模式就忽略不说了, 这里在系统上还有两个可能会**导致效率低下**的地方: **很多小的I/O操作**和**过多的字节拷贝**。

##### 1、避免小I/O问题
在客户端和服务器端都会发生, 在服务器端有它自己的存储操作。

为了避免这个问题, **我们的通讯协议这是基于消息集合这个概念构建的**, 很容易把多个消息组合起来. 这样允许网络组合消息后进行发送, 而不是每次发送一条信息, 减少网络的来回开销. 服务器也是每次写入一堆数据到日志中, 消费者也是每次线性读取一堆数据。

**这种简单的优化可以提升大量的性能, 批量处理导致大的网络数据包, 大的磁盘顺序读写, 连续的内存块等等, 所有的这些能把kafka的间接性的随机消息写改成线性写入后, 发送给消费者。**

##### 2、避免过多的字节拷贝
**另外一个低效率的地方是字节拷贝**。在消息吞吐量不多的时候这不是一个问题，但**在高负载下的影响是非常显著**。**为了避免这种情况，我们在生产者、服务器和消费者间使用一个标准化的二进制消息格式（这样数据块可以在它们之间直接进行传输而不需要再做修改）**。

**服务器端broker使用文件的形式维护消息日志, 所有的消息都按生产者和消费者使用的格式顺序写入到磁盘中, 维护这样的格式需要优化最常用的一些操作: 对持久日志块的网络传输。 现代unix操作系统通常都有提供高效的优化代码直接把数据从缓存页发送到socket； 在linux下使用sendfile的系统调用**。

**如果要理解一下sendfile调用的功效, 需要了解下正常情况下数据从文件发送到socket的过程（具体可以查看操作系统的IO）**：
1. 操作系统从磁盘读取数据到系统内核空间的缓存页中；
2. 应用通过系统调用，从内核空间读取数据到用户空间缓冲区中；
3. 应用通过系统调用，把数据写回到内核空间的socket缓冲区中；
4. 操作系统拷贝socket缓冲区的数据到网卡缓冲区, 然后由网卡发送数据到网络中。

**这很明显很没效率, 有4次拷贝还有2次系统调用。如果使用sendfile命令, 系统直接把数据从缓存页拷贝到网络, 所以最终只需要一次从缓存页到网卡缓冲区拷贝（zero-copy）**。

我们预期一个常见消费方式是使用多个消费者同时消费一个topic。 **使用zero-copy的优化方式**, 数据只被拷贝到页缓存一次, 并被多次消费, 而不是缓存到(用户空间的)内存中, 然后在每次读取消费时拷贝到系统内核空间中. **这可以使消费者消费消息的速度达到网络连接的速度**。

**组合页缓存和sendfile机制后, kafka集群在跟上消费者消费的同时, 让你觉得好像没有多少的磁盘读活动, 因为大部分的数据响应需求都是从缓存获取的。**

如果想要知道更**多关于java对sendfile和zero-copy的支持**, 可以阅读这篇文章：https://developer.ibm.com/articles/j-zerocopy/。（Java 对于sendfile和zero-copy的支持.md）

##### 3、端到端批量压缩
**在大部分情况下, 效率瓶颈不会是cpu或磁盘, 而是网络带宽**。这个在数据中心之间建立需要跨越广域网发送消息的数据管道时更为明显, 当然用户可以独立于kafka自己做消息压缩, 但是这有可能由于消息类型冗余, 导致压缩比例很低(例如, json的字段名, 或web中的用户代理日志, 或常用的字符串值), **有效的压缩方式应该是允许压缩重复的消息, 而不是分别压缩单个消息**。

kafka通过递归的消息集合支持这样的操作：**一批的消息可以被收集在一起后压缩, 并发送到服务器端。这样被压缩的一批数据, 在日志也是使用压缩的格式, 只有在消费者消费的时候才会被解压**。

kafka支持 GZIP, Snappy and LZ4 压缩协议, 更多关于压缩的细节可以查看这里：https://cwiki.apache.org/confluence/display/KAFKA/Compression。

#### 发布者（Producer）
**发布者的设计思想主要体现在负载均衡和异步发送两方面。**
##### 负载均衡（Load balancing）
**为了让生产者实现这个功能, 所有的kafka服务器节点都能响应这样的元数据请求: 哪些服务器是活着的, Topic的哪些分区是主分区, 分配在哪个服务器上, 这样发布者就能适当地直接发送它的请求到服务器上。**

生产者发送数据到主分区的服务器上, 不需要经过任何中间路由。

客户端控制消息发送数据到哪个分区, 这个可以实现随机的负载均衡方式. 或者使用一些特定语义的分区函数, 我们有提供特定分区的接口让用于**根据指定的键值进行hash分区**(当然也有选项可以重写分区函数), 例如, 如果键值使用用户ID, 则用户相关的所有数据都会被分发到同一个分区上. 这允许消费者, 在消费数据时做一些特定的本地化处理. 这样的分区风格经常被设计用于一些本地处理比较敏感的消费者。

##### 异步发送（Asynchronous send）
批处理是提升性能的一个主要驱动, 为了允许批量处理, kafka发布者会尝试在内存中汇总消息, 并用一次请求批次提交信息。批处理, 不仅仅可以配置指定的消息数量, 也可以指定等待特定的延迟时间(如64k 或10ms), 这允许汇总更多的数据后再发送, 在服务器端也会减少更多的IO操作。 该缓冲是可配置的，并给出了一个机制，通过权衡少量额外的延迟时间获取更好的吞吐量。

#### 消费者（Consumer）
消费者通过从主分区的服务器获取数据进行消费。消费者指定每次请求时日志的偏移量, 然后从这个位置开启批量获取数据. 消费者对位移量有绝对的控制权, 这样消费者可以重新设置位移位置, 并在有需要的时重新消费。

##### 1、推送vs拉取
一个基本的问题是, 我们在考虑, 消费者是否主动从服务器那里拉取数据, 还是服务器应该主动推送数据到消费者端。

在这方面,** kafka和传统的消息吸引设计一样, 生产者推送消息到服务器, 消费者从服务器拉取消息**。在一些日志中心系统, 像Scribe and Apache Flume, 使用一种特殊的推送流数据推送机制, 这些方式都有利有弊, 但是, 在一个基于推送方式消息系统, 很难处理大量的消费者, 因为服务器需要控制数据的传输速率。 目标是为了让消费者尽可能多消费数据;不幸的是，在一个推送系统，这意味着消费者往往被消息淹没，如果消费率低于生产速度(例如密集的服务攻击)。基于拉去的系统往往比较优雅些, 消息处理只是落后, 消费者在后面尽可能赶上。

**使用基于拉取方式的系统还有一个好处就是容易汇集批量数据后发给消费者**。基于推送的系统, 要么马上发送请求, 要么汇总数据后再发送, 而不管下游的消费者是否能够处理得上。 如果为了进一步降低延迟, 这会导致缓存还没有结束时就传输单条数据过去, 这样很浪费. 基于拉的方式可以从当前日志位置拉取可用的消息(或者根据配置的大小)。 这样能在没有引入不必要的延迟的情况下, 获取到比较好的批处理性能。

**基于拉取方式的系统不足的地方是如果没有任何数据, 消费者就要循环检测, 使用空轮询的繁忙检测方式等候数据到**来。为了避免这一点，我们可以设置拉请求的参数，允许消费者请求在“长轮询”时阻塞，直到数据到达（并且可选地等待直到给定数量的字节可用以确保大的传输大小）。

你可以想象一些其他从端到端的一些可行性设计。生产者把记录写入到本地日志中, 服务器（brokers）将从消费者拉取的数据中拉取。一种类似的储存和转发的生产者模型经常被提议。 这虽然挺有趣的, 但不适合有成千上万生产者的情况. 在我们大规模运行数据储存系统的经验来看, 成千上万的磁盘跨越多个应用并不让系统更为可靠, 操作起来将会是一个噩梦。 在实践中, 我们发现可以创建具有很强壮的SLAs保障的, 大规模的管道, 并且不需要提供者有持久化能力。

##### 2、消费者消费定位
**令人惊讶的是，跟踪已消费的消息是消息传递系统的关键性能点之一。**

大部分的消息系统在服务器端维护元数据信息用来记录哪些消息被消费。也就是, 当消息被发送给消费者时, 服务器要么立即在本地记录日志, 要么等待消费者反馈后记录。这样并不直观, 事实上，对于单服务器, 很难理清楚这个状态到底去哪里了。因为在大部分的消息储存系统中, 数据结构很难被扩展, 这也是一个实用的选择--因为如果服务器知道消息被消费后可以马上删除, 那么就可以维持比较小的数据集。

**可能不太明显的是, 让broker和consumer对已经消费的数据达成一致并不是一件简单的事情**。如果broker在每次数据分发出去后, 马上标记消息已经被消费了, 如果消费者处理消息失败了(例如宕机了), 那么消息可能会丢失。为了解决这个问题, 很多消息系统添加了反馈机制, 用于标记消息已经被发送, 而不是被消费, 服务器等待消费者发送一个反馈来确认消息已经被消费。这个策略解决消息丢失的问题, 但是同时也引发新的问题。
- 首先, 如果消费者已经消费了记录, 但是在反馈时失败, 则有可能重复消费两次。
- 其次, 是多一个来回的性能损耗, 现在服务器就要为每个消息保存不同的状态(先锁定, 这样不会发送第二次, 然后标记为永久消费后, 才能把它删除)。
- 还有些麻烦的问题需要处理, 比如消息被发送了, 但是从来没有接受到反馈。

Kafka的解决方式：
- kafka使用不一样的处理方式, **Topic被划分成一系列有序的分区集合, 每个分区在一个时刻仅被订阅消费者组中的一个消费者消费**. 这意味这每个消费者在一个分区位置就只是一个数值, 用于记录下一次消息要被消费的位置. 这意味着记录消费者状态的代价非常小, 只是每个分区一个数值. 这个状态可以定期做检查点, 这使等价的消息反馈代价非常小。
- 这个方案还有另外的好处, **消费者可以优雅地重新指定一个旧的位移位置, 并重新消费数据**. 这个和通常的队列观念有点相悖, 但是对很多消费者来说是一个很重要的特性. 例如，如果消费代码有bug，并且在一些消息被消费后发现，一旦bug被修复，消费者可以重新使用这些消息。

##### 3、离线数据加载（Offline Data Load）
可扩展的持久性存储能力, 使得消费者能定期批量把数据导入到离线系统中, 如:Hadoop 或关系型数据仓库。

在Hadoop的情况下，我们通过将负载分配到各个映射任务来并行化数据负载，每个节点/主题/分区组合一个，允许加载中的完全并行。 Hadoop提供任务管理，失败的任务可以在没有重复数据危险的情况下重新启动 - 它们只是从原始位置重新启动。

##### 4、静态成员（Static Membership）
**静态成员资格旨在提高流应用程序、消费者组和基于组重新平衡协议构建的其他应用程序的可用性**。
- **重新平衡协议（The rebalance protocol）**依赖于组协调器将实体ID分配给组成员。这些生成的id是短暂的，当成员重新启动并重新加入时会发生变化。
- 对于基于消费者的应用程序，此“动态成员资格（dynamic membership）”可能会导致在管理操作（例如代码部署，配置更新和定期重新启动）期间将大部分任务重新分配给不同的实例。 

对于大型状态应用程序，洗牌任务需要很长时间才能在处理之前恢复其本地状态，并导致应用程序部分或完全不可用。受此现象的启发，**Kafka的群组管理协议允许群组成员提供持久性实体ID。 根据这些ID，组成员资格保持不变，因此不会触发重新平衡。**

如果要使用静态成员资格：
- 将broker集群和客户端应用程序升级到2.3或更高版本，并确保升级的broker也使用2.3或更高版本的inter.broker.protocol.version。
- 将config ConsumerConfig＃GROUP_INSTANCE_ID_CONFIG设置每个消费者实例的唯一值在一个组下。
- 对于Kafka Streams应用程序，为每个KafkaStreams实例设置唯一的ConsumerConfig＃GROUP_INSTANCE_ID_CONFIG就足够了，与实例的已使用线程数无关。

如果您的代理的版本低于2.3，但您选择在客户端设置ConsumerConfig＃GROUP_INSTANCE_ID_CONFIG，则应用程序将检测代理版本，然后抛出UnsupportedException。 如果您不小心为不同的实例配置了重复的ID，则代理端的防护机制会通过触发org.apache.kafka.common.errors.FencedInstanceIdException来通知您的重复客户端立即关闭。

#### 消息传递分发语义
现在我们大致理解生产者和消费者是怎么工作的, 现在我们讨论下**kafka提供的基于生产者和消费者之间提供的保障机制. 这里有三种消息的发送保障机制**。
- At most once—最多一次, 消息可能会丢失, 但是不会被重复分发。
- At least once—至少一次, 消息不会丢失, 但有可能会重复分发。
- Exactly once—有且仅有一次, 这是人们最终想要的, 消息仅且只会被分发一次。

值得注意的是, 这会拆分成两个问题：**发布消息可用性的保障**和**消费消息可用性的保障**。

Kafka的语义很直接。**在发布消息时，我们有一个消息被“提交”到日志的概念**。 **一旦已发布的消息被提交了，只要复制写入此消息的分区的一个broker保持“alive”，它就不会丢失**。 提交消息的定义，活动分区以及我们尝试处理哪些类型的故障的描述将在下一节中更详细地描述。现在让我们假设一个完美无损的broker，并尝试对生产者和消费者提供保障。 如果生产者尝试发布消息并遇到网络错误，则无法确定在提交消息之前或之后是否发生了此错误。这个类似于使用插入数据到数据库时使用自动增长的主键的情形。
- 在0.11.0.0之前，如果生产者未能收到消息已提交的响应，则除了重新发送消息之外别无选择。 这提供至少一次传递语义，因为如果原始请求实际上成功，则在重新发送期间可以再次将消息写入日志。
- 从0.11.0.0开始，**Kafka生成器还支持幂等传递选项**，该选项保证重新发送不会在日志中导致重复条目。 为实现此目的，broker为每个生产者分配一个ID，并使用生产者发送的序列号和每条消息对消息进行重复数据删除。
- 同样从0.11.0.0开始，**生产者支持使用类似事务的语义向多个主题分区发送消息的能力**：即，所有消息都被成功写入，或者都没有。 这方面的主要用例是Kafka主题之间的一次性处理（如下所述）。

并不是所有的情况都需要这样强的保障的。对于那些对延迟比较敏感的生产者, 我们允许生产者自定义可用性级别。比如生产者愿意等候消息10ms后再被提交。然而, 生产者也可以配置完全使用异步发送, 或者等候到主服务器(而不是所有的副本)已经拥有这份消息。

**现在让我们从消费者的角度来描述语义**。所有副本都具有完全相同的日志，具有相同的偏移量，消费者控制其在此日志中的位置。 如果消费者永远不会崩溃，它只能将此位置存储在内存中，但如果消费者失败并且我们希望该主题分区被另一个进程接管，则新进程将需要选择适当的位置以开始处理。假设消费者阅读一些消息 - 它有几个选项来处理消息并更新其位置。
- **它可以先读取消息，然后将其位置保存在日志中，最后处理消息**。在这种情况下，消费者进程可能在保存其位置之后但在保存其消息处理的输出之前崩溃。 在这种情况下，接管处理的过程将从保存的位置开始，即使该位置之前的一些消息尚未处理。这对应于“最多一次”语义，因为在消费者失败的情况下可能不会处理消息。
- **它可以先读取消息，然后处理消息，最后保存其位置**。在这种情况下，消费者进程可能在处理消息之后但在保存其位置之前崩溃。 在这种情况下，当新进程接管它收到的前几条消息时，它们已经被处理过了。这对应于消费者失败情况下的“至少一次”语义。在许多情况下，消息具有主键，因此更新是幂等的（两次接收相同的消息只是用另一个自身的副本覆盖记录）。

**那么Exactly once语义（即你真正想要的东西）呢**？ 
1. 当从Kafka Topic消费并生成另一个Topic时（如在Kafka Streams应用程序中），我们可以利用上面提到的0.11.0.0中的新事务生成器功能。
2. 消费者的偏移位置作为Topic中的一条消息存储，因此我们可以在与接收处理数据的输出Topic相同的事务中将偏移量写入Kafka。
3. 如果事务中止，则消费者的位置将恢复为其旧值，并且输出Topic上生成的数据将不会被其他消费者看到，具体取决于其“隔离级别”。 
4. 在默认的“read_uncommitted”隔离级别中，消费者可以看到所有消息，即使它们是中止事务的一部分，但在“read_committed”中，消费者只会从已提交的事务中返回消息（和那些不在事务中的消息）。
##### 写入外部系统时的处理
在写入外部系统时，限制是需要协调消费者的位置与实际存储的输出。**实现这一目标的经典方法是在消费者位置的存储和消费者输出的存储之间引入两阶段提交**。 

**但是这可以通过让消费者将其偏移存储在与其输出相同的位置来更简单地处理**。这是更好的，**因为消费者可能想要写入的许多输出系统并不支持两阶段提交**。

这里有一个例子，考虑一个Kafka Connect连接器，它填充HDFS中的数据以及它读取的数据的偏移量，以确保数据和偏移量都得到更新或两者都不更新。 我们遵循许多其他数据系统的类似模式，这些数据系统需要这些更强的语义，并且消息没有主键以允许重复数据删除。

因此，Kafka支持Kafka Streams中的一次性交付，并且在Kafka主题之间传输和处理数据时，事务性生产者/消费者通常可用于提供一次性交付。 对其他目标系统的一次性交付通常需要与此类系统合作，但Kafka提供了偏移，这使得实现这一点成为可行（另请参见Kafka Connect）。 **否则，Kafka默认保证至少一次交付，并允许用户通过在处理一批消息之前禁用生产者的重试并在消费者中提交偏移来实施最多一次交付**。

#### 复制（Replication）
**Kafka为每个Topic的分区日志设计了复制规则：将分区日志复制到一个可配置的数据的服务器集群上(你可以对每个Topic设置副本数)。这保证了如果集群中有服务器宕机时能够自动恢复, 消息可以从剩余的服务器中读取。保证了消息的可用性。**

其他消息系统也提供复制特性, 但是, 在我们(有点偏见地)看来, 这是一个附加的特性, 不能大量使用, 并且伴随大量的缺点：备机不是活跃的、吞吐量严重受到影响、还需要繁琐的人工配置等等。 kafka默认开启复制功能, 实际上我们把没有实现复制的Topic当作只有一个副本的Topic来看待。

**复制的基本单元是基于主题分区（topic partition）**。在非故障的情况下, 每个分区在kafka中有1个Leader和0个或多个followers分区, 所有的这些follwer包括leader构成了复制因子。所有读写都转移到Leader分区。正常情况下, 分区数量一般比broker多的多, 所有的Leader分区最终平均分布到所有的服务器上。 在follower服务器上的日志一般和Leader服务器的日志一致, 拥有相同的偏移量和消息顺序(当然, 在特定的时间内, Leader分区日志的末尾有一些尚未复制的消息)。

Follower像正常的Kafka消费者一样消费来自leader的消息并将其应用于他们自己的日志。这些Follower服务器有个很好的特性, 就是能自然地获取批量数据并应用到他们自己的日志中。

##### "Alive"节点判定
与大多数分布式系统一样，自动处理故障需要准确定义节点“Alive”的含义。对于Kafka节点，活跃度有两个条件：
- 节点必须能够与ZooKeeper维护其会话（通过ZooKeeper的心跳机制）；
- 如果它是一个Follower节点，它必须在主节点写的时候进行复制, 不能落下太远。

我们称满足这两个条件的节点为**”in sync”(在同步中)**, 避免使用”alive”或”failed”这中模糊的术语。主节点保持跟踪同步中的节点, 如果一个备份节点宕机, 卡住, 或跟不上, 主节点将会把它从已经同步的复制集中删除, 用于判定卡住或者落后延迟, **使用replica.lag.time.max.ms这个配置参数**。

在一个分布式的术语里, 我们尝试处理”失败/恢复”模型, 像节点突然停止工作, 然后又恢复的(可能不知道他们是否宕机了)。 kafka不处理所谓的“拜占庭”故障，比如节点产生任意或恶意的反馈(比如bug或不规范行为)。

现在，**我们可以更精确地定义当该分区的所有同步副本将消息应用于其日志时，将消息视为已提交**。
- 只有已提交的消息才会发给消费者，这意味着消费者不必担心如果领导者失败可能会看到丢失的消息。
- 另一方面，生产者可以选择是否等待消息的发送，这取决于他们在延迟和持久性之间进行权衡的偏好。此首选项由生产者使用的acks设置控制。请注意，Toppic具有同步副本“最小数量”的设置，当生产者请求确认已将消息写入完整的同步副本集时，将检查该副本。如果生产者请求不那么严格的确认，则即使同步副本的数量低于最小值（例如，它可以仅低于领导者），也可以提交和消费该消息。

**Kafka提供的保证是，只要始终存在至少一个同步副本，就不会丢失已提交的消息**。

在短暂的故障转移期后，Kafka将在出现节点故障时仍然可用，但在网络分区存在时可能无法保持可用状态。

##### 复制日志：Quorums，ISR和State Machines
**Kafka分区的核心是复制日志**。复制日志是分布式数据系统中最基本的原语之一，有许多实现方法。其他系统可以使用复制日志作为基元，以状态机实现分布式。

> 在计算机科学中， 状态机复制或状态机方法是通过复制服务器和协调客户端与服务器副本的交互来实现容错服务的一般方法。该方法还提供了用于理解和设计复制管理协议的框架。状态机从标记为“开始”的状态开始。接收到的每个输入都通过转换和输出函数传递，以产生新的状态和输出。在收到新输入之前，状态保持稳定，而输出则传送给适当的接收器。（https://en.wikipedia.org/wiki/State_machine_replication）

**复制日志模型用于处理连续输入的, 有序的记录值**(通常将日志条目编号为：1,2,3,...)。 这里有很多实现的方式, 但是最简单和最有效的方式是：**主节点选择和提供这个顺序值，只要主节点存活, 备份节点只要按主节点选择的顺序拷贝这些值就可以了。**

当然，如果领导者没有失败，我们就不需要追随者了！当领导者故障时，我们需要从追随者中选择一位成为新的领导者。但是追随者本身可能落后或崩溃所以我们必须确保我们选择一个最新的追随者。 **日志复制算法必须提供的基本保证是：如果我们告诉客户端消息已提交，并且领导者失败，我们选择的新领导者也必须拥有该消息**。 这产生了一个权衡：**如果领导者在宣布承诺之前等待更多的追随者承认消息，那么将会有更多潜在的可选领导者**。

如果你选择那些需要反馈的数量和可以用于选举为主节点的日志数可以保证重叠, 这个叫做**Quorum**。

**一种达到这种目标的最常用的方法是, 在提交决策和选举决策上都使用最多投票的方式**, 这不是kafka的实现, 但是我们为了明白这个原理还是解释下：如果说我们有2f+1个副本, 那么f+1的副本必须在主节点提交日志前接受到消息, 这样我们就可以从拥有最完全的日志的f+1个副本集中选择出主服务器。因为在任何f+1个副本中, 肯定有一个副本是包含全部的日志的, 这个副本的日志是最新的, 因此会被选择为主节点. 这里有很多关于这个算法的细节需要处理(像如果定义使日志更全些, 再主节点宕机时保证日志的一致性, 修改复制集中日志的副本数 ), 但是我们现在先忽略。

这种多数投票方法具有非常好的属性：**延迟仅取决于最快的服务器**。也就是说，如果复制因子是3，则延迟由较快的跟随者而不是较慢的跟随者确定。

该系列中有各种各样的算法，包括ZooKeeper的Zab，Raft和Viewstamped Replication。 我们了解到Kafka实际实施的最相似的学术出版物是Microsoft的PacificA。

**多数投票的缺点是，它并不容忍很多的失败**, 导致你没有可被选择为主节点的备节点。为了容忍1个错误需要3份的副本数据, 要容忍3个失败需要5份副本数据. 在我们的经验中, 用足够多的冗余来来容忍单一的错误在现实中的系统是不够的, 这样每次写5次, 使用5被的硬盘空间和5份之一的带宽, 在大体量的数据储存上不是特别实践, 所以quorum的算法机制在共享的集群配置中好像更为常见写, 但是在主储存结构上比较少用, 例如, hdfs的namenode节点使用基于主副本的选举机制建立高可用性能, 但是由于代价太高没有用在数据方面。

**kafka使用有点儿不太一样的策略来选择他的quorum集合。不像多数投票一样, kafka动态维护能跟得上主节点的复制集合(ISR), 只有在这个集合里面的成员才有资格被选举为主节点, 这个对于kafka这种拥有很多分区并且需要保证主节点的负载均衡的模型来说非常重要。使用ISR这样的模型和f+1个副本, kafka的主题可以容忍f个节点宕机后已经提交的消息不会丢失。**

**另一个重要的设计区别是Kafka不要求崩溃的节点在其所有数据完整的情况下恢复**。此空间中的复制算法依赖于“稳定存储”的存在并不罕见，“稳定存储”在任何故障恢复方案中都不会丢失而没有潜在的一致性违规。这个假设有两个主要问题。首先，磁盘错误是我们在持久数据系统的实际操作中观察到的最常见问题，并且它们通常不会保持数据完整。其次，即使这不是问题，我们也不希望在每次写入时都要求使用fsync来保证一致性，因为这会将性能降低两到三个数量级。我们允许副本重新加入ISR的协议确保在重新加入之前，它必须完全重新同步，即使它在崩溃中丢失了未刷新的数据。

#### 不清楚主选举:如果全部宕机了?（Unclean leader election: What if they all die? ）
**注意, kafka只有在isr集合至少有一个副本时, 才能保障数据不会丢失, 如果一个分区的所有节点都宕机了, 就保证不了了。**

尽管这样, 在现实中的系统, 需要做一些事情在所有副本都宕机的情况下。如果你不幸遇到了, 你需要仔细考虑下将碰到的问题。有两种行为需要去做:
- 等待ISR中一个副本恢复, 然后选择这个副本作为主节点(期望数据不会丢失)；
- 选择第一个存活的副本(不一定在ISR副本集中)直接作为主节点。

这个必须在可用性和一致性之间作权衡：
- 如果我们等待在ISR集合中的副本再次启动起来, 那么在所有副本及都宕机这段时间, 我们会维持不可用的状态。如果这些副本已经坏了, 或对应的数据已经丢失了, 则我们永久宕机了。
- 如果, 换种方式, 从没有同步的副本中选择一个存活的变成主的, 那么这个副本的日志就变成当前主要的数据源, 但是保证当前所有已经提交的消息还存在。默认情况下, kafka使用第二种策略, 在ISR中的所有副本集都宕机时, 使用一个潜在的非一致性的副本,如果我们更期望是不可用状态而不是不一致状态时, 这个特性可以通过配置unclean.leader.election.enable来禁用。

这种困境不是kafka特有的, 这存在于任何基于quorum方式的结构中。例如, 多数投票算法, 如果大多数的服务器都永久性失效了, 你必须选择丢失全部的数据或者接受某一台可能数据不一致的服务器上的数据。

#### 可用性和可靠性保证（Availability and Durability Guarantees）
**在写入Kafka时，生产者可以选择是否等待消息被0,1或所有（-1）个副本确认。**

**请注意，“所有副本的确认”并不保证已分配的完整副本集已收到该消息**。 默认情况下，当acks = all时，只要所有当前的同步内副本都收到消息，就会发生确认。 例如，如果主题仅配置了两个副本而一个失败（即，同步副本中只有一个仍然存在），则指定acks = all的写入将成功。但是，如果剩余的副本也失败，则这些写入可能会丢失。 虽然这确保了分区的最大可用性，但对于喜欢耐用性而非可用性的一些用户来说，这种行为可能是不合需。因此，**我们提供了两种主题级配置，可用于优先考虑消息的持久性和可用性**：
1. 禁止无主选举, 如果所有的副本都不可用, 这个分区就要等到最近一个主分区可以用时才可用, 这可能导致不可用, 而不是数据丢失；
2. 指定一个最小的ISR集合, 分区只有在ISR集合的个数大于指定值时, 才能进行读写, 这样可以阻止消息只写入到一个副本的, 随后这个副本宕机导致数据丢失。这个只有参数生产者开启了全反馈的时才能保证消息会在所有同步的副本集中至少有这么多个反馈。这个参数提供了一致性和可用性之前的权衡, 较大最小ISR可以保证比较好的一致性, 因为消息被写入更多的副本, 减少丢失的可能性, 但是同时也减低了可用性, 因为分区的副本数如果达不到最小ISR集合时将不可用。

#### 复制管理（Replica Management）
上面讨论的复制日志实际上只涉及到单个日志, 即一个主题分区。但是, kafka集群管理成百上千这样的分区. 我们用轮询的方式对分区进行负载分摊, 避免一个主题的所有分区都被集聚到几个节点上。 同样的我们要分摊平衡主节点, 这样每个服务器上承载的主分区节点都有一定的比例。

**对不可用的时间端, 优化主节点的选举也很重要**。**一个直观的选举实现是如果一个节点宕机了, 那么这个节点上的每个分区都独立选举**。

但是, 我们选举一个节点作为控制器。这个控制器在节点级别上进行容错管理, 和负责修改所有的被影响的分区的选举。这样好处是,我们可以批量处理选举, 减少很多独立选举时大量通知, 这使得在大量分区时选举代价更小, 更快。如果这个控制器失败了, 其中一个还存活的节点会变成主控制器。

#### 配额（Quotas）
Kafka集群能够对请求实施配额，以控制客户端使用的broker资源。Kafka代理可以为共享配额的每组客户强制执行两种类型的客户配额：
- **网络带宽配额**，定义字节速率阈值（自0.9起）；
- **请求速率配额**，将CPU利用率阈值定义为网络和I/O线程的百分比（自0.11起）。

##### 为什么需要配额？
生产者和消费者可能生成/消费非常大量的数据或以非常高的速率生成请求，从而垄断代理资源，导致网络饱和，并且通常是DOS其他客户端和代理本身。 具有配额可以防止这些问题，并且在大型多租户群集中更为重要，其中一小部分行为不端的客户端会降低用户体验的性能。 事实上，当将Kafka作为服务运行时，这甚至可以根据商定的合同强制执行API限制。

##### 客户端分组
**Kafka客户端的标识是用户主体，它表示安全集群中经过身份验证的用户**。在无鉴权机制的集群中, 用户身份是由服务器使用可配置的PrincipalBuilder进行选择的, Client-id作为客户端逻辑分组, 是由客户端应用选择的一个有意义的名称。元组（user，client-id）定义了一个共享用户主体和客户端ID的安全逻辑客户端组。

Quotas可以应用于（user，client-id）、user或client-id组。对于给定连接，将最符合这个连接的配额被使用到，配额组的所有连接都共享为该组配置的配额。例如，如果（user =“test-user”，client-id =“test-client”）的产品配额为10MB /秒，这个配置会被所有的具有”test-user”用户和客户端ID是 “test-client”的所有生产者所共享。

##### 配额如何配置
**配额可以按照(user, client-id)或者, user或client-id进行分组, 如果需要更高或更低的配额, 可以覆盖默配额, 这个机制类似于对日志主题配置的覆盖, user 或者 (user, client-id)配额可以覆盖写入到zookeeper下的 /config/users ,而client-id配置可以写入到 /config/clients**。这些覆盖写入会被服务器很快的读取到, 这让我们修改配置不需要重新启动服务器。 每个分组的默认配置也可以同样的方式动态修改。

配额配置的优先顺序是：
1. /config/users/<user>/clients/<client-id>
2. /config/users/<user>/clients/<default>
3. /config/users/<user>
4. /config/users/<default>/clients/<client-id>
5. /config/users/<default>/clients/<default>
6. /config/users/<default>
7. /config/clients/<client-id>
8. /config/clients/<default>

##### 网络带宽配额（Network Bandwidth Quotas）
**网络带宽配额定义为共享配额的每组客户端的字节速率阈值**。 

默认情况下，每个唯一客户端组都会收到由群集配置的固定配额（以字节/秒为单位）。此配额是基于每个broker定义的。 在客户端受到限制之前，每组客户端可以在每个broker上发布/消费最多X字节/秒。

##### 请求速率配额（Request Rate Quotas）
**请求率配额定义为客户端可以在配额窗口内的每个代理的请求处理程序I/O线程和网络线程上使用的时间百分比。**

n％的配额表示一个线程的n％，因此配额超出（（num.io.threads + num.network.threads）* 100）％的总容量。在受到限制之前，每组客户端可以在配额窗口中的所有I / O和网络线程中使用高达n％的总百分比。由于为I/O和网络线程分配的线程数通常基于broker主机上可用的核心数，因此请求速率配额表示共享配额的每组客户端可以使用的CPU的总百分比。

##### 配额方案实施
**默认情况下，每个唯一客户端组都会收到群集配置的固定配额，此配额是基于每个broker定义的，每个客户端可以在受到限制之前使用每个broker的配额**。 我们决定为每个broker定义这些配额比为每个客户端定义一个固定的群集宽带宽要好得多，因为这需要一种机制来在所有代理之间共享客户端配额使用。这比配额实施本身更难做到！

**broker在检测到配额违规时如何反应？**

在我们的解决方案中，代理首先计算将违规客户端置于其配额下所需的延迟量，并立即返回具有延迟的响应。如果是获取请求，则响应将不包含任何数据。然后，代理将通道静音到客户端，不再处理来自客户端的请求，直到延迟结束。在收到具有非零延迟持续时间的响应时，Kafka客户端还将在延迟期间避免向代理发送进一步的请求。因此，来自受限节点的客户端的请求被双方有效地阻止。即使较旧的客户端实现不尊重来自代理的延迟响应，代理通过静音其套接字通道所施加的反压仍然可以处理对性能不佳的客户端的限制。那些向限制频道发送进一步请求的客户只有在延迟结束后才会收到回复。

在多个小窗口（例如，每个1秒的30个窗口）上测量字节速率和线程利用率，以便快速检测和纠正配额违规。通常，具有大的测量窗口（例如，每个30秒的10个窗口）导致大的流量突发，随后是长延迟，这在用户体验方面不是很大。
### log compaction
日志压缩保证了在同一个主题分区中, 最终保留每条消息相同主键最后一个值。例如在应用程序崩溃或系统故障后恢复状态，或在运行维护期间重新启动应用程序后重新加载缓存。

到目前为止，我们仅描述了更简单的数据保留方法，其中旧的日志数据在固定的时间段之后或当日志达到某个预定大小时被丢弃。这适用于时间事件数据，例如每个记录独立的日志记录。但是，一类重要的数据流是对键控的可变数据的更改日志（例如，对数据库表的更改）。

让我们讨论一下这种流的具体例子。假设我们有一个包含用户电子邮件地址的主题每次用户更新其电子邮件地址时，我们都会使用其用户ID作为主键向此主题发送消息。现在说我们在一段时间内为id为123的用户发送以下消息，每条消息对应于电子邮件地址的更改（其他ID的消息被省略）：
```
123 => bill@microsoft.com
        .
        .
        .
123 => bill@gatesfoundation.org
        .
        .
        .
123 => bill@gmail.com
```
日志压缩提供我们更多的日志保存机制, 我们可以保证每个主键的的最后一条记录都会保留(如: bill@mail.com), 这样做, 我们能保证日志拥有每个键的最终值的完整镜像, 下游的消费者在从主题恢复状态的时候也不需要消费完整的日志信息。

让我们看下这个功能在那些情况下有用, 然后我们在看能够怎么被使用:
- 数据库变更订阅。
- 事件源。
- 高可用日志。

**这些用例中主要需要处理实时系统的实时反馈, 但是有时, 机器宕机, 数据需要重新加载或重新处理, 有时要全部加载。**

**日志压缩提供对每条记录的保存方式提供细粒度的机制, 而不是基于时间范围的粗款的方式**。我们可以在对具有相同主键的记录更新时, 选择性删除记录.这样日志可以保证拥有每个键值的最后状态。

这种保存策略可以针对Topic基本设置, 这样一个集群的一些Topic可以按大小和时间进行保存, 一些可以按压缩策略进行保存。

#### 日志压缩基础（Log Compaction Basics）
下图显示了Kafka日志的逻辑结构以及每条消息的偏移量：

![image](https://note.youdao.com/yws/res/19146/8AC909B9E06A4E268AF7C86795FF7207)

**日志的头部具有密集的连续偏移并保留所有消息**。日志压缩添加了一个用于处理日志尾部的选项。上图显示了压缩尾部的日志。
- 请注意，日志尾部的消息保留了首次写入时分配的原始偏移量 - 从不更改。 
- 另请注意，这些消息的位移位置即使在压缩过后也是合法的, 在这种情况下, 这个位置和下次出现在日志中的最高位移位置是很难区分的。 
例如，在上图中，偏移36,37和38都是等效位置，并且从这些偏移中的任何一个开始的读取将返回以38开始的消息集。

**压缩同时也允许删除**, 如果一个带键值的消息没有任何负载数据会被认为是要从日志中删除记录, 这个删除标志会导致先前带有这个键值的消息都被删除。但是删除标志比较特殊, 在过一段时期后会被清除后释放空间。这个执行删除的时间点, 标记为**”删除保留点”**。

**日志压缩在后台以定时拷贝日志段的方式进行** 。清理不会阻止读取，并且可以限制使用不超过可配置的I/O吞吐量，以避免影响生产者和使用者。 实际的日志段压缩过程有点像如下:

![image](https://note.youdao.com/yws/res/19163/47C42F2A61EB4878B6A4FC6EAB3F6B28)

#### 日志压缩提供了什么保障?
日志压缩提供了如下保障:
- **任何留在日志头部内的消费者都会看到所写的每条消息;这些消息将具有连续的偏移量**。参数min.compaction.lag.ms可以控制消息必须经过质指定的时间间隔后才能被压缩, 它提供了一个消息可以储存多久在头部的短暂时间范围；主题的max.compaction.lag.ms可用于保证消息写入时间与消息符合压缩条件的时间之间的最大延迟。
- 消息顺序永远被保证, 压缩不会重新排序, 只会删除一些；
- 消息的偏移量永远不会改变。它是日志中位置的永久标识符。
- 任何从头开始消费记录的消费者, 都会按顺序得到最终的状态。所有的删除标志的记录会在消费者到达头部之前, 小于主题设置的 delete.retention.ms(默认是24小时)时间之内被处理.这个在删除标志发生在并行读之前很重要, 这样我们可以保证我们在消费者读取之前没有删除任何标志。

#### 日志压缩细节
日志压缩由日志清理程序（log cleaner）处理，日志清理程序是一个后台线程池，用于重新复制日志段文件，删除其密钥出现在日志头部的记录。 每个压缩程序线程的工作原理如下：
1. 它选择日志头到日志尾比率最高的日志；
2. 它为日志头部中的每个键创建了最后一个偏移量的简洁摘要；
3. 它从头到尾重新记录日志，删除在日志最后出现的键值。新的、干净的日志段会立即交换到日志中，因此所需的额外磁盘空间只是一个额外的日志段（不是日志的完整副本）；
4. 日志头部汇总实际上是一个空间紧凑的hash表, 使用24个字节一个条目的形式, 所以如果有8G的整理缓冲区, 则能迭代处理大约366G的日志头部(假设消息大小为1k，8*1024*1024*1024/24 * 1014 = 366GB)

#### log cleaner 配置
默认情况下启用日志清理程序。 这将启动更清晰的线程池。 要对特定主题启用日志清理，请添加特定于日志的属性

```
log.cleanup.policy=compact
```
log.cleanup.policy属性是代理的server.properties文件中定义的代理配置设置; 它会影响群集中没有配置覆盖的所有主题，如此处所述。日志清理程序可以配置为保留最小量的日志的未压缩“头”。 通过设置压缩时间延迟来启用此功能。

```
log.cleaner.min.compaction.lag.ms
```
这个可以用于防止消息比当前正在压缩的最小消息时间更新, 如果没有设置, 所有的日志都会压缩, 除了最后一个正在被读写的段. 当前段甚至在消息大于最小压缩延迟时间也不会被压缩。         

```
log.cleaner.max.compaction.lag.ms
```
这可以用于防止具有低生产率的日志在无限期的持续时间内不能进行压缩。如果未设置，则不会压缩不超过min.cleanable.dirty.ratio的日志。 请注意，此压缩截止日期并非难以保证，因为它仍然受到日志清理程序线程和实际压缩时间的影响。 您需要监视uncleanable-partitions-count，max-clean-time-secs和max-compaction-delay-secs指标。

### 基于设计思想的实施
#### 网络层（Network Layer）
网络层是一个相当直接的NIO服务器，不再详细描述。

**sendfile实现是通过给MessageSet接口提供writeTo方法来完成的。这允许文件支持的消息集使用更高效的transferTo实现而不是进程内缓冲写入。（实现zero-copy提高性能）**

**线程模型**是：**单个接受器线程**和**N个处理器线程**，每个线程处理固定数量的连接。这种设计在其他地方经过了相当全面的测试，发现它易于实现且速度快。协议保持非常简单，以便将来使用其他语言实现客户端。

#### 消息（Messages）
消息的组成是：
- 可变长度头；
- 可变长度且不透明的key字节数组；
- 可变长度且不透明的value字节数组。

消息头的格式将在后面介绍。**保留key和value不透明是正确的决定**：目前在序列化库上取得了很大进展，任何特定的选择都不太适合所有用途。 毋庸置疑，使用Kafka的特定应用程序可能会强制使用特定的序列化类型作为其使用的一部分。 

**RecordBatch接口只是消息的迭代器，具有用于批量读取和写入NIO通道的专用方法**。

#### 消息格式（Message Format）
**消息（即记录）始终分批写入**。

一批消息的技术术语是记录批次（record batch），而记录批次包含一个或多个记录。在消极的情况下，我们可以有一个包含单个记录的记录批次。记录批次和记录都有自己的头部。 每种格式如下所述。

##### 记录批次（Record Batch）
以下是RecordBatch的磁盘格式。
（请注意，启用压缩后，压缩记录数据将在记录数量计数后直接序列化。）
```
baseOffset: int64
batchLength: int32
partitionLeaderEpoch: int32
magic: int8 (current magic value is 2)
crc: int32
attributes: int16
    bit 0~2:
        0: no compression
        1: gzip
        2: snappy
        3: lz4
        4: zstd
    bit 3: timestampType
    bit 4: isTransactional (0 means not transactional)
    bit 5: isControlBatch (0 means not a control batch)
    bit 6~15: unused
lastOffsetDelta: int32
firstTimestamp: int64
maxTimestamp: int64
producerId: int64
producerEpoch: int16
baseSequence: int32
records: [Record]
```
CRC覆盖从属性到批处理结束的数据（即CRC之后的所有字节）。它位于magic字节之后，这意味着客户端必须在决定如何解释批处理长度和magic字节之间的字节之前解析magic字节。分区前导符号字段不包括在CRC计算中，以避免在为代理接收的每个批次分配该字段时重新计算CRC的需要。CRC-32C（Castagnoli）多项式用于计算。

**压缩**：与旧的消息格式不同，magic v2及更高版本在清理日志时保留原始批次中的第一个和最后一个偏移/序列号。这是必需的，以便能够在重新加载日志时恢复生产者的状态。例如，如果我们没有保留最后的序列号，那么在分区引导程序失败之后，生产者可能会看到OutOfSequence错误。必须保留基本序列号以进行重复检查（代理通过验证传入批次的第一个和最后一个序列号与该生产者的最后一个序列号匹配来检查传入的重复生成请求）。因此，当清理批次中的所有记录但仍保留批次以保留生产者的最后序列号时，可以在日志中包含空批次。这里有一个奇怪的地方，在压缩过程中不会保留baseTimestamp字段，因此如果批处理中的第一条记录被压缩，它将会改变。

###### 控制批次
控制批处理包含一个称为**控制记录**的记录。控制记录不应传递给应用程序，相反，消费者使用它们来过滤掉中止的事务性消息。

控制记录的键符合以下模式：
```
version: int16 (current version is 0)
type: int16 (0 indicates an abort marker, 1 indicates a commit)
```
控制记录值的模式取决于类型。该值对客户端不透明。

##### 记录（Record）
记录级Header是在Kafka 0.11.0中引入的。带有Header的记录的磁盘格式如下所示。

```
length: varint
attributes: int8
    bit 0~7: unused
timestampDelta: varint
offsetDelta: varint
keyLength: varint
key: byte[]
valueLen: varint
value: byte[]
Headers => [Header]
```
###### 记录头（Record Header）

```
headerKeyLength: varint
headerKey: String
headerValueLength: varint
Value: byte[]
```
我们使用与Protobuf相同的varint编码。 关于后者的更多信息可以在[这里](https://developers.google.com/protocol-buffers/docs/encoding#varints)找到。记录中的头部数也被编码为varint。

#### 日志（Log）
**包含两个partition，名称为“my_topic”的Topic的日志包含两个目录（名称为my_topic_0和my_topic_1）,其中包含该Topic的消息的数据文件**。

日志文件的格式是**log entry**的条目。
- 每个日志条目是一个4字节整数N，存储消息长度，后跟N个消息字节的消息数据。
- 每条消息都有一个64位的offset标识这条消息在这个Topic的Partition中的偏移量。
- 每个日志文件都以它存储的第一条消息的offset命名。所以第一个文件会命名为00000000000.kafka，随后每个文件的文件名将是前一个文件的文件名加上S的正数，S是配置中指定的单个文件的大小。

**消息的确切的二进制格式都有版本，它保持一个标准的接口，让消息集可以根据需要在Producer、Broker、Consumer之间传输而不需要重新拷贝或者转换**。

**使用消息偏移量作为消息ID是不常见的**。我们初始的想法是在Producer生成一个GUID作为Message ID，并在Broker上维持ID和Offset之间的映射关系。但是因为Consumer需要为每个Server维持一个ID，那么GUID的全局唯一性就变得没什么意义了。此外，维持一个随机的ID和Offset的映射关系将给索引的构建带来巨大的负担，本质上需要一个完整的持久化的随机存取的数据结构。因此，为了简化查找结构，我们决定使用每个分区的原子计数器，它可以和分区ID加上ServerID来唯一标识一条消息。一旦使用了计数器，直接使用Offset进行跳转是顺其自然的，两者都是分区内单调递增的整数。由于偏移量从消费者API中隐藏起来，因此这个决定是最终的实现细节，所以我们采用更有效的方法。

![image](https://note.youdao.com/yws/res/19383/4B66173015E34F67B49DFC0971F5F76F)

##### 写（Writes）
该日志允许串行追加始终转到最后一个文件。 当文件达到可配置的大小（比如1GB）时，该文件将转移到一个新文件。

该日志有两个配置参数：
- M，配置达到多少条消息后进行刷盘；
- S，配置多长时间之后进行刷盘。 

**这个持久化策略保证最多只丢失M条消息或者S秒之内的消息**。


##### 读（Reads）
**读取通过提供64位的offset和S-byte的chunk大小来实现**。

这将返回包含在S-byte的buffer的消息迭代器。S比任意单条消息都大，但是如果在异常的超大消息的情况下，读取操作可以通过多次重试，每次都将buffer大小翻倍，直到消息被读取成功。最大消息大小和buffer大小可以配置，用于拒绝超过特定大小的消息，以限制客户端读取消息时需要拓展的buffer大小。buffer可能以不完整的消息作为结尾，这可以通过消息大小来轻松的检测到。

实际的读取操作首先需要定位offset所在的文件，再将offset转化为文件内相对的偏移量，然后从文件的这个偏移量开始读取数据。搜索操作通过内存中映射的文件的简单的二分查找完成。

日志提供了获取最近写入消息的能力以允许客户端从“当前时间”开始订阅。这在客户端无法在指定天数内消费掉消息的场景中非常有用。在这种情况下，如果客户端尝试消费一个不存在的offset将抛出OutOfRangeException异常并且可以根据场景重置或者失败。

以下是发送给消费者的数据格式：
```
MessageSetSend (fetch result)
 
total length     : 4 bytes
error code       : 2 bytes
message 1        : x bytes
...
message n        : x bytes
```

```
MultiMessageSetSend (multiFetch result)
 
total length       : 4 bytes
error code         : 2 bytes
messageSetSend 1
...
messageSetSend n
```
##### 删除（Delete）
**数据一次删除一个日志段**。

日志管理器允许可配置删除策略选择哪些文件可以删除。**当前策略删除修改时间超过N天的任何日志，但保留最后N GB的策略也可能有用**。

**为了避免删除时锁定读取操作，我们采用copy-on-write的方式来实现，以保证一致性的视图。**

##### 担保（Guarantees）
**志提供了配置参数M，用于控制写入多少条消息之后进行一次刷盘**。在日志恢复过程中遍历最新的日志段的所有消息并验证每一条消息是有效的。如果消息的大小和偏移量之和小于文件长度并且消息的CRC32和存储的CRC相同，那么消息是有效的。在异常事件被检测到时，日志会被截取到最后一条有效的消息的offset。

请注意，**必须处理两种损坏**：
- 因为Crash导致的written块丢失；
- 无意义的block被添加到文件中。

这样做的原因是，一般的操作系统不保证file inode和实际数据块之间的写入顺序，所以除了丢失丢失written data，文件还会获得无意义的数据，在inode更新大小但是在block写入数据之前。CRC检测这个错误并防止损坏日志（unwritten的消息肯定会丢失）。

#### 分布式（Distribution）
##### 消费者偏移追踪
**Kafka消费者跟踪它在每个分区中消耗的最大偏移量，并且能够提交偏移量，以便在重新启动时可以从这些偏移量中恢复**。 Kafka提供了在指定的broker（针对该组）中将给定消费者组的所有偏移存储为**组协调器（ group coordinator）**的选项。即，**该消费者组中的任何消费者实例应将其偏移提交和提取发送给该group coordinator（broker）**。消费者组根据其组名分配给coordinator。消费者可以通过向任何Kafka代理发出FindCoordinatorRequest并读取包含协调器详细信息的FindCoordinatorResponse来查找其协调器。然后，消费者可以继续从协调代理程序提交或获取偏移量。在coordinator移动的情况下，消费者将需要重新发现coordinator 。偏移提交可以由消费者实例自动或手动完成。

**当group coordinator收到OffsetCommitRequest时，它会将请求附加到名为__consumer_offsets的特殊压缩Kafka主题**。仅在偏移主题的所有副本都接收到偏移量后，broker才会向消费者发送成功的偏移提交响应。如果偏移量在可配置的超时内无法复制，则偏移提交将失败，并且消费者可以在后退后重试提交。broker定期压缩偏移主题，因为它只需要维护每个分区的最新偏移提交。**协调器还将偏移缓存在内存表中，以便快速提供偏移提取。**
```
root@node5:/tmp/kafka-logs# ls
applet-click-test-0        __consumer_offsets-0   __consumer_offsets-17  __consumer_offsets-25  __consumer_offsets-33  __consumer_offsets-41  __consumer_offsets-5              test-0
apple-test-0               __consumer_offsets-1   __consumer_offsets-18  __consumer_offsets-26  __consumer_offsets-34  __consumer_offsets-42  __consumer_offsets-6 
```

当coordinator接收到偏移量获取请求时，它只返回来自偏移量高速缓存的最后提交的偏移量向量。如果coordinator刚刚启动或者它刚刚成为新的一组消费者组的coordinator（通过成为偏移主题分区的领导者），它可能需要将偏移主题分区加载到缓存中。在这种情况下，偏移量提取将失败并出现CoordinatorLoadInProgressException，并且消费者可能会在退回后重试OffsetFetchRequest。

##### ZooKeeper目录存储信息
以下给出了用于consumer和broker之间协调的ZooKeeper结构和算法。

###### 符号声明（Notation）
当路径中的元素表示为[xyz]时，这意味着xyz的值不是固定的，并且实际上对于xyz的每个可能值存在ZooKeeper znode。 例如/topics/[topic]将是一个名为/ topics的目录，其中包含每个主题名称的子目录。 还给出了数值范围，例如[0 ... 5]，以指示子目录0,1,2,3,4。箭头->用于指示znode的内容。例如/hello -> world表示包含值“world”的znode / hello。

###### broker节点
节点路径格式：

```
/brokers/ids/[0...N] --> {"jmx_port":...,"timestamp":...,"endpoints":[...],"host":...,"version":...,"port":...} (ephemeral node)
```
举例说明：

```
[zk: 127.0.0.1:2181(CONNECTED) 1] ls /brokers/ids
[0]
[zk: 127.0.0.1:2181(CONNECTED) 2] get /brokers/ids/0
{"listener_security_protocol_map":{"PLAINTEXT":"PLAINTEXT"},"endpoints":["PLAINTEXT://192.168.100.248:9092"],"jmx_port":-1,"host":"192.168.100.248","timestamp":"1563521322190","port":9092,"version":4}

```
**这是所有当前brokers节点的列表，每个代理节点都提供一个唯一的逻辑代理ID，用于向消费者标识它（必须作为其配置的一部分提供）。在启动时，broker节点通过在/broker/ids下创建逻辑代理ID的znode来注册自身。逻辑代理ID的目的是允许将broker移动到不同的物理机器而不影响使用者。尝试注册已在使用的代理ID（例如，因为两个服务器配置了相同的代理ID）会导致错误。**

由于代理使用临时znode在ZooKeeper中注册自己的节点，因此该注册是动态的，并且如果broker关闭或死亡（因此通知消费者不再可用），会话消失则该临时节点将消失。

###### broker对Topic的注册
节点路径格式：

```
/brokers/topics/[topic]/partitions/[0...N]/state --> {"controller_epoch":...,"leader":...,"version":...,"leader_epoch":...,"isr":[...]} (ephemeral node)

```
每个broker在其维护的Topic下注册自己，并存储该Topic的分区数。

举例：

```
[zk: 127.0.0.1:2181(CONNECTED) 5] ls /brokers/topics
[TestT, TestWordCount, __consumer_offsets, apple-test, applet-click-test, applet-item-click-test, applet-item-click-test1, applet-item-test, applet-test, applet-test1, applet-test2, test, userPortraits]
[zk: 127.0.0.1:2181(CONNECTED) 6] ls /brokers/topics/applet-test2/partitions
[0]
[zk: 127.0.0.1:2181(CONNECTED) 7] get /brokers/topics/applet-test2/partitions/0/state
{"controller_epoch":4,"leader":0,"version":1,"leader_epoch":0,"isr":[0]}

```
###### Cluster Id 集群id
**集群标识是分配给Kafka集群的唯一且不可变的标识符**。群集ID最多可包含22个字符，允许的字符由正则表达式[a-zA-Z0-9 _ \ - ] +定义，它对应于URL安全的Base64变体使用的字符，没有填充。 从概念上讲，它是第一次启动集群时自动生成的。

在实现方面，它是在第一次成功启动版本为0.10.1或更高版本的代理时生成的。代理尝试在启动期间从/cluster/id znode获取集群标识。如果znode不存在，代理将生成新的集群标识，并使用此集群标识创建znode。

```
[zk: 127.0.0.1:2181(CONNECTED) 1] ls /cluster/id
[]
[zk: 127.0.0.1:2181(CONNECTED) 2] get /cluster/id
{"version":"1","id":"vLOn4d6pTqayU1w48B-QgQ"}

```
###### broker节点注册
broker节点基本上是独立的，因此它们只发布有关它们的内容的信息。当broker加入时，它会在broker节点注册表目录下注册自己，并写入有关其主机名和端口的信息。 broker还在broker Topic注册表中注册现有Topic及其逻辑分区的列表。在代理上创建新Topic时，会动态注册这些Topic。

#### 一般操作运维行为（Operation）
##### 优雅的关闭（Graceful shutdown）
Kafka群集将自动检测任何broker关闭或故障，并为该机器上的分区选择新的leader。无论服务器发生故障还是故意将其关闭以进行维护或配置更改，都会发生这种情况。 对于后一种情况，Kafka支持更优雅的机制来停止服务器，而不仅仅是杀死服务器。当服务器正常停止时，它有两个优化，它将利用：
1. 它会将所有日志同步到磁盘，以避免在重新启动时需要进行任何日志恢复（即验证日志尾部所有消息的校验和）。日志恢复需要时间，因此加速了故意重启。
2. 在关闭之前，它会将服务器所领先的任何分区迁移到其他副本。这将使领导层转移更快，并将每个分区不可用的时间缩短到几毫秒。

每当服务器停止而不是硬杀死时，将自动同步日志，但受控领导迁移需要使用特殊设置：

```
controlled.shutdown.enable=true
```
请注意，只有在broker上托管的所有分区都具有副本（即复制因子大于1并且这些副本中至少有一个处于活动状态）时，受控关闭才会成功。 这通常是您想要的，因为关闭最后一个副本会使该主题分区不可用。

##### Balancing leadership
每当broker停止或崩溃领导时，该broker的分区转移到其他副本。这意味着默认情况下，当broker重新启动时，它将只是所有分区的follower，这意味着它不会用于客户端读取和写入。

为了避免这种不平衡，Kafka有一个首选副本的概念。如果分区的副本列表是1,5,9，则节点1首选作为节点5或9的领导者，因为它在副本列表中较早。您也可以让Kafka群集通过运行命令尝试恢复已恢复副本的领导：
```
> bin/kafka-preferred-replica-election.sh --zookeeper zk_host:port/chroot
```
由于运行此命令可能很繁琐，您还可以通过设置以下配置来配置Kafka自动执行此操作：
```
auto.leader.rebalance.enable=true
```
##### Balancing Replicas Across Racks

##### 集群之间数据镜像复制
我们指的是在Kafka集群之间复制数据“镜像”的过程，以避免与单个集群中的节点之间发生的复制混淆。 Kafka附带了一个用于在Kafka集群之间镜像数据的工具。 该工具从源群集使用并生成到目标群集。 这种镜像的一个常见用例是在另一个数据中心提供副本。

##### 检查消费者的消费位置
有时检查消费者的位置很有用。 我们有一个工具可以显示消费者群体中所有消费者的位置以及他们所在日志的结尾。 要在名为my-group的使用者组中运行此工具，请使用名为my-topic的主题，如下所示：

```
> bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group
 
TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID                                       HOST                           CLIENT-ID
my-topic                       0          2               4               2          consumer-1-029af89c-873c-4751-a720-cefd41a669d6   /127.0.0.1                     consumer-1
my-topic                       1          2               3               1          consumer-1-029af89c-873c-4751-a720-cefd41a669d6   /127.0.0.1                     consumer-1
my-topic                       2          2               3               1          consumer-2-42c1abd4-e3b2-425d-a8bb-e1ea49b29bb2   /127.0.0.1                     consumer-2

```
##### 管理消费者组
使用ConsumerGroupCommand工具，我们可以列出，描述或删除consumer group。 可以手动删除consumer group，也可以在该组的最后一次提交的偏移量到期时自动删除。 仅当组没有任何活动成员时，手动删除才有效。 例如，列出所有Topic的所有使用者组：

```
> bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
 
test-consumer-group
```
要查看偏移量，如前所述，我们“describe”这样的消费者群体：

```
> bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group
 
TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                    HOST            CLIENT-ID
topic3          0          241019          395308          154289          consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2
topic2          1          520678          803288          282610          consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2
topic3          1          241018          398817          157799          consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2
topic1          0          854144          855809          1665            consumer1-3fc8d6f1-581a-4472-bdf3-3515b4aee8c1 /127.0.0.1      consumer1
topic2          0          460537          803290          342753          consumer1-3fc8d6f1-581a-4472-bdf3-3515b4aee8c1 /127.0.0.1      consumer1
```
还有许多其他“describe”选项可用于提供有关消费者组的更详细信息：
- --members：此选项提供使用者组中所有活动成员的列表。

```
> bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group --members
 
CONSUMER-ID                                    HOST            CLIENT-ID       #PARTITIONS
consumer1-3fc8d6f1-581a-4472-bdf3-3515b4aee8c1 /127.0.0.1      consumer1       2
consumer4-117fe4d3-c6c1-4178-8ee9-eb4a3954bee0 /127.0.0.1      consumer4       1
consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2       3
consumer3-ecea43e4-1f01-479f-8349-f9130b75d8ee /127.0.0.1      consumer3       0
```
- --members --verbose：除了上面“--members”选项报告的信息之外，此选项还提供分配给每个成员的分区。

```
> bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group --members --verbose
 
CONSUMER-ID                                    HOST            CLIENT-ID       #PARTITIONS     ASSIGNMENT
consumer1-3fc8d6f1-581a-4472-bdf3-3515b4aee8c1 /127.0.0.1      consumer1       2               topic1(0), topic2(0)
consumer4-117fe4d3-c6c1-4178-8ee9-eb4a3954bee0 /127.0.0.1      consumer4       1               topic3(2)
consumer2-e76ea8c3-5d30-4299-9005-47eb41f3d3c4 /127.0.0.1      consumer2       3               topic2(1), topic3(0,1)
consumer3-ecea43e4-1f01-479f-8349-f9130b75d8ee /127.0.0.1      consumer3       0               -
```
- --offsets：这是默认的describe选项，提供与“--describe”选项相同的输出。
- --state：此选项提供有用的组级信息。

```
> bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-group --state
 
COORDINATOR (ID)          ASSIGNMENT-STRATEGY       STATE                #MEMBERS
localhost:9092 (0)        range                     Stable               4
```
要手动删除一个或多个使用者组，可以使用“--delete”选项：

```
> bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --delete --group my-group --group my-other-group
 
Deletion of requested consumer groups ('my-group', 'my-other-group') was successful.
```
要重置使用者组的偏移，可以使用“--reset-offsets”选项。 此选项当时支持一个使用者组。 它需要定义以下范围： -  all-topics或--topic。 除非使用“--from-file”方案，否则必须选择一个范围。 此外，首先确保使用者实例处于非活动状态。 有关详细信息，请参阅KIP-122。

它有3个执行选项：
- （默认）显示要重置的偏移量。
- --execute：执行--reset-offsets进程。
- --export：将结果导出为CSV格式。

--reset-offsets还有以下场景可供选择（必须选择至少一个场景）：

```
--to-datetime <String：datetime>：将偏移重置为datetime的偏移量。 格式：'YYYY-MM-DDTHH：mm：SS.sss'
--to-earliest：将偏移重置为最早的偏移量。
--to-latest：将偏移重置为最新偏移。
--shift-by <Long：number-of-offsets>：重置偏移将当前偏移量移位'n'，其中'n'可以是正数或负数。
--from-file：将偏移重置为CSV文件中定义的值。
--to-current：将偏移重置为当前偏移。
--by-duration <String：duration>：重置偏移量以从当前时间戳的持续时间偏移。 格式：'PnDTnHnMnS'
--to-offset：将偏移重置为特定偏移量。
```
##### 扩展集群
**将服务器添加到Kafka集群很简单，只需为它们分配一个唯一的代理ID，并在新服务器上启动Kafka**。 但是，这些新服务器不会自动分配任何数据分区，因此除非将分区移动到它们，否则在创建新主题之前它们不会执行任何工作。 因此，通常在将计算机添加到群集时，您需要将一些现有数据迁移到这些计算机。

迁移数据的过程是手动启动的，但完全自动化。 在幕后，Kafka将添加新服务器作为其正在迁移的分区的跟随者，并允许它完全复制该分区中的现有数据。 当新服务器完全复制此分区的内容并加入同步副本时，其中一个现有副本将删除其分区的数据。

**分区重新分配工具**可用于在broker之间移动分区。 **理想的分区分布将确保所有broker的均衡数据负载和分区大小**。 **分区重新分配工具无法自动研究Kafka群集中的数据分布并移动分区以实现均匀的负载分配。 因此，管理员必须弄清楚应该移动哪些主题或分区。**

分区重新分配工具可以在3种互斥模式下运行：
- --generate：在此模式下，给定topics列表和brokers列表，该工具会生成候选重新分配，以将指定主题的所有分区移动到新代理。 此选项仅提供了一种方便的方法，可在给定主题和目标代理列表的情况下生成分区重新分配计划。
- --execute：在此模式下，该工具根据用户提供的重新分配计划启动分区的重新分配。 （使用--reassignment-json-file选项）。 这可以是由管理员手工制作的自定义重新分配计划，也可以使用--generate选项提供
- --verify：在此模式下，该工具将验证最后一次--execute期间列出的所有分区的重新分配状态。 状态可以是成功完成，失败或正在进行中。

###### 自动将数据迁移到新计算机
**分区重新分配工具**可用于将一些topics从当前的broker集移动到新添加的broker。这在扩展现有集群时通常很有用，因为将整个Topic移动到新的broker集更容易，而不是一次移动一个分区。 当用于执行此操作时，用户应提供应移至新的broker集的topics列表和新broker的目标列表。

然后，该工具在新的broker集中均匀分配给定Topics列表的所有分区。在此移动期间，Topic的复制因子保持不变。有效地，输入Topics列表的所有分区的副本将从旧的broker集移动到新添加的broker。

例如，以下示例将主题foo1，foo2的所有分区移动到新的代理集5,6。 在此移动结束时，主题foo1和foo2的所有分区将仅存在于代理5,6上。

由于该工具接受主题的输入列表作为json文件，因此首先需要确定要移动的主题并创建json文件，如下所示：

```
> cat topics-to-move.json
{"topics": [{"topic": "foo1"},
            {"topic": "foo2"}],
"version":1
}
```
一旦json文件准备就绪，使用分区重新分配工具生成候选分配：

```
> bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file topics-to-move.json --broker-list "5,6" --generate
Current partition replica assignment
 
{"version":1,
"partitions":[{"topic":"foo1","partition":2,"replicas":[1,2]},
              {"topic":"foo1","partition":0,"replicas":[3,4]},
              {"topic":"foo2","partition":2,"replicas":[1,2]},
              {"topic":"foo2","partition":0,"replicas":[3,4]},
              {"topic":"foo1","partition":1,"replicas":[2,3]},
              {"topic":"foo2","partition":1,"replicas":[2,3]}]
}
 
Proposed partition reassignment configuration
 
{"version":1,
"partitions":[{"topic":"foo1","partition":2,"replicas":[5,6]},
              {"topic":"foo1","partition":0,"replicas":[5,6]},
              {"topic":"foo2","partition":2,"replicas":[5,6]},
              {"topic":"foo2","partition":0,"replicas":[5,6]},
              {"topic":"foo1","partition":1,"replicas":[5,6]},
              {"topic":"foo2","partition":1,"replicas":[5,6]}]
}
```
该工具生成一个候选分配，将所有分区从主题foo1，foo2移动到代理5,6。 但请注意，此时分区移动尚未开始，它只是告诉您当前的分配和建议的新分配。 应保存当前分配，以防您想要回滚它。 新的赋值应保存在json文件（例如expand-cluster-reassignment.json）中，并使用--execute选项输入到该工具，如下所示：

```
> bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --execute
Current partition replica assignment
 
{"version":1,
"partitions":[{"topic":"foo1","partition":2,"replicas":[1,2]},
              {"topic":"foo1","partition":0,"replicas":[3,4]},
              {"topic":"foo2","partition":2,"replicas":[1,2]},
              {"topic":"foo2","partition":0,"replicas":[3,4]},
              {"topic":"foo1","partition":1,"replicas":[2,3]},
              {"topic":"foo2","partition":1,"replicas":[2,3]}]
}
 
Save this to use as the --reassignment-json-file option during rollback
Successfully started reassignment of partitions
{"version":1,
"partitions":[{"topic":"foo1","partition":2,"replicas":[5,6]},
              {"topic":"foo1","partition":0,"replicas":[5,6]},
              {"topic":"foo2","partition":2,"replicas":[5,6]},
              {"topic":"foo2","partition":0,"replicas":[5,6]},
              {"topic":"foo1","partition":1,"replicas":[5,6]},
              {"topic":"foo2","partition":1,"replicas":[5,6]}]
}
```
最后，--verify选项可与该工具一起使用，以检查分区重新分配的状态。 请注意，相同的expand-cluster-reassignment.json（与--execute选项一起使用）应与--verify选项一起使用：

```
> bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json --verify
Status of partition reassignment:
Reassignment of partition [foo1,0] completed successfully
Reassignment of partition [foo1,1] is in progress
Reassignment of partition [foo1,2] is in progress
Reassignment of partition [foo2,0] completed successfully
Reassignment of partition [foo2,1] completed successfully
Reassignment of partition [foo2,2] completed successfully
```

###### 自定义分区和转移
分区重新分配工具还可用于选择性地将分区的副本移动到特定的broker集。当以这种方式使用时，假设用户知道重新分配计划并且不需要工具生成候选重新分配，有效地跳过 - 生成步骤并直接移动到--execute步骤

例如，以下示例将主题foo1的分区0移动到代理5,6，将主题foo2的分区1移动到代理2,3：

第一步是在json文件中手工制作自定义重新分配计划：

```
> cat custom-reassignment.json
{"version":1,"partitions":[{"topic":"foo1","partition":0,"replicas":[5,6]},{"topic":"foo2","partition":1,"replicas":[2,3]}]}
```
然后，使用带有--execute选项的json文件来启动重新分配过程：

```
> bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --execute
Current partition replica assignment
 
{"version":1,
"partitions":[{"topic":"foo1","partition":0,"replicas":[1,2]},
              {"topic":"foo2","partition":1,"replicas":[3,4]}]
}
 
Save this to use as the --reassignment-json-file option during rollback
Successfully started reassignment of partitions
{"version":1,
"partitions":[{"topic":"foo1","partition":0,"replicas":[5,6]},
              {"topic":"foo2","partition":1,"replicas":[2,3]}]
}
```
--verify选项可与该工具一起使用，以检查分区重新分配的状态。 请注意，相同的expand-cluster-reassignment.json（与--execute选项一起使用）应与--verify选项一起使用：

```
> bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file custom-reassignment.json --verify
Status of partition reassignment:
Reassignment of partition [foo1,0] completed successfully
Reassignment of partition [foo2,1] completed successfully
```

##### 移除broker
分区重新分配工具无法自动生成退役经纪人的重新分配计划。 因此，管理员必须提出重新分配计划，以便将代理上托管的所有分区的副本移动到其他代理。 这可能相对繁琐，因为重新分配需要确保所有副本不会从退役的代理移动到仅一个其他代理。 为了使这一过程毫不费力，我们计划在未来为退役经纪人添加工具支持。

##### 增加复制因子
增加现有分区的复制因子很容易。 只需在自定义重新分配json文件中指定额外副本，并将其与--execute选项一起使用，以增加指定分区的复制因子。

例如，以下示例将主题foo的分区0的复制因子从1增加到3。在增加复制因子之前，分区的唯一副本存在于broker 5上。作为增加复制因子的一部分，我们将添加更多副本broker 6和7。

第一步是在json文件中手工制作自定义重新分配计划：

```
> cat increase-replication-factor.json
{"version":1,
"partitions":[{"topic":"foo","partition":0,"replicas":[5,6,7]}]}
```
然后，使用带有--execute选项的json文件来启动重新分配过程：

```
> bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --execute
Current partition replica assignment
 
{"version":1,
"partitions":[{"topic":"foo","partition":0,"replicas":[5]}]}
 
Save this to use as the --reassignment-json-file option during rollback
Successfully started reassignment of partitions
{"version":1,
"partitions":[{"topic":"foo","partition":0,"replicas":[5,6,7]}]}
```
--verify选项可与该工具一起使用，以检查分区重新分配的状态。 请注意，相同的increase-replication-factor.json（与--execute选项一起使用）应与--verify选项一起使用：

```
> bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --verify
Status of partition reassignment:
Reassignment of partition [foo,0] completed successfully

```
您还可以使用kafka-topics工具验证复制因子的增加：
```
> bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic foo --describe
Topic:foo   PartitionCount:1    ReplicationFactor:3 Configs:
  Topic: foo    Partition: 0    Leader: 5   Replicas: 5,6,7 Isr: 5,6,7
```

##### 限制数据迁移期间的带宽使用
Kafka允许您对复制流量应用限制，设置用于将副本从一台机器移动到另一台机器的带宽的上限。 这在重新平衡群集，引导新代理或添加或删除代理时非常有用，因为它限制了这些数据密集型操作对用户的影响。

有两种方式：最简单，最安全的是在调用kafka-reassign-partitions.sh时应用节流，但kafka-configs.sh也可用于直接查看和更改节流值。

因此，例如，如果您要执行重新平衡，使用以下命令，它将以不超过50MB / s的速度移动分区。
```
$ bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file bigger-cluster.json —throttle 50000000
```
详细见：https://kafka.apache.org/documentation/#operations

还要注意安全的使用限制工具

##### 设定配额
https://kafka.apache.org/documentation/#operations

#### 数据中心
某些部署需要管理跨多个数据中心的数据管道。 我们建议的方法是在每个数据中心部署一个本地Kafka集群，每个数据中心中的应用程序实例仅与其本地集群交互，并在集群之间进行镜像（有关如何执行此操作，请参阅镜像制造商工具的文档）。

此部署模式允许数据中心充当独立实体，并允许我们集中管理和调整数据中心间复制。 这样，即使数据中心间链路不可用，每个设施也可以独立运行：当发生这种情况时，镜像会落后，直到链路恢复，此时它会赶上。

对于需要所有数据的全局视图的应用程序，您可以使用镜像来提供具有从所有数据中心中的本地群集镜像的聚合数据的群集。 这些聚合簇用于需要完整数据集的应用程序的读取。

这不是唯一可能的部署模式。 可以通过WAN读取或写入远程Kafka群集，但显然这将增加获取群集所需的延迟。

**Kafka自然地在生产者和消费者中批量处理数据，因此即使在高延迟连接上也可以实现高吞吐量**。为此，尽管可能需要使用socket.send.buffer.bytes和socket.receive.buffer.bytes配置来增加生产者，使用者和Broker的TCP套接字缓冲区大小。 这里记录了设置此方法的适当方法。

**通常不建议在高延迟链路上运行跨越多个数据中心的单个Kafka群集**。这将导致Kafka写入和ZooKeeper写入的非常高的复制延迟，并且如果位置之间的网络不可用，则Kafka和ZooKeeper都不会在所有位置保持可用。
#### Kafka配置
**最重要的生产者配置**是：
- acks
- compression
- batch size

最重要的消费者配置是fetch size（获取大小）。

[配置部分](https://kafka.apache.org/documentation/#configuration)中记录了所有配置。

##### 生产服务器配置
以下是生产服务器配置示例：

```
# ZooKeeper服务器配置
zookeeper.connect=[list of ZooKeeper servers]
 
# Log configuration
# 每个主题的默认日志分区数
num.partitions=8
# 自动创建主题的默认复制因子
default.replication.factor=3
# 保存日志数据的目录（对于log.dirs属性的补充）
log.dir=[List of directories. Kafka should have its own dedicated disk(s) or SSD(s).]
 
# Other configurations
# 此服务器的代理ID。如果未设置，将生成唯一的代理ID。为了避免zookeeper生成的代理ID和用户配置的代理ID之间发生冲突，生成的代理ID从reserved.broker.max.id + 1开始。
broker.id=[An integer. Start with 0 and increment by 1 for each new broker.]
# 监听器列表（安全模块），合法侦听器列表的示例：PLAINTEXT：// myhost：9092，SSL：//：9091 CLIENT：//0.0.0.0：9092，REPLICATION：// localhost：9093
listeners=[list of listeners]
# 是否在服务器上启用Topic的自动创建
auto.create.topics.enable=false
# 当生产者将acks设置为“all”（或“-1”）时，min.insync.replicas指定必须确认写入被认为成功的最小副本数。
min.insync.replicas=2
# 在阻塞网络线程之前，允许数据平面的队列请求数
queued.max.requests=[number of concurrent requests]
```
我们的客户端配置在不同用例之间变化很大。

#### JVM调优
从安全角度来看，我们建议您使用最新发布的JDK 1.8版本，因为较旧的免费版本已经披露了安全漏洞。 LinkedIn目前正在使用G1收集器运行JDK 1.8 u5（希望升级到更新版本）。 LinkedIn的调整看起来像这样：

```
-Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC
-XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M
-XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80
```
作为参考，以下是LinkedIn最繁忙的一个集群（最高峰）的统计数据：
- 60 brokers
- 50k partitions (replication factor 2)
- 800k messages/sec in
- 300 MB/sec inbound, 1 GB/sec+ outbound

调整看起来相当激进，但该集群中的所有broker都有90％的GC暂停时间约为21毫秒，并且它们每秒的GC运行时间不到1个。

#### 硬件和操作系统
我们正在使用具有24GB内存的双四核Intel Xeon机器。您需要足够的内存来缓冲活动的读者和编写者。假设您希望能够缓冲30秒并将内存需求计算为write_throughput * 30，则可以对内存需求进行背面估计。

磁盘吞吐量很重要。通常，磁盘吞吐量是性能瓶颈，更多磁盘更好。根据您配置刷新行为的方式，您可能会或可能不会从更昂贵的磁盘中受益（如果您经常强制刷新，那么更高的RPM SAS驱动器可能会更好）。

##### 操作系统优化要求
Kafka应该可以在任何unix系统上运行良好，并且已经在Linux和Solaris上进行了测试。

它不太可能需要很多操作系统级别的调整，但有三种可能重要的操作系统级别配置：
- **文件描述符限制**：**Kafka使用文件描述符来记录日志段和打开连接**。 如果代理托管多个分区，请考虑代理至少需要（number_of_partitions）*（partition_size / segment_size）来跟踪除代理所建立的连接数之外的所有日志段。 我们建议为broker进程至少提供100000个允许的文件描述符作为起点。注意：mmap（）函数添加对与文件描述符fildes关联的文件的额外引用，该文件描述符fildes不会被该文件描述符上的后续close（）删除。 当没有更多映射到文件时，将删除此引用。
- **最大套接字缓冲区大小**：可以增加以在数据中心之间实现高性能数据传输，如[此处所述](http://www.psc.edu/index.php/networking/641-tcp-tune)。
- **进程可能具有的最大内存映射区域数（也称为vm.max_map_count）**。 请参阅Linux[内核文档](http://kernel.org/doc/Documentation/sysctl/vm.txt)。 在考虑代理可能具有的最大分区数时，您应该关注此OS级属性。 **默认情况下，在许多Linux系统上，vm.max_map_count的值大约为65535**。每个分区分配的每个日志段需要一对index / timeindex文件，并且每个文件占用1个映射区域。 换句话说，每个日志段使用2个映射区域。 因此，每个分区至少需要2个映射区域，只要它承载单个日志段即可。 也就是说，在broker上创建50000个分区将导致分配100000个映射区域，并可能导致代理在具有默认vm.max_map_count的系统上因OutOfMemoryError（映射失败）而崩溃。 请记住，**每个分区的日志段数取决于段大小，负载强度，保留策略，并且通常倾向于多于一个**。

##### 磁盘和文件系统
我们建议使用多个驱动器以获得良好的吞吐量，并且不与应用程序日志或其他OS文件系统活动共享用于Kafka数据的相同驱动器，以确保良好的延迟。您可以将这些驱动器一起RAID为单个卷或格式，并将每个驱动器作为其自己的目录安装。由于Kafka具有复制功能，因此RAID提供的冗余也可以在应用程序级别提供。这种选择有几个权衡。

如果配置多个数据目录，则会将分区循环分配给数据目录。每个分区将完全位于其中一个数据目录中。如果数据在分区之间没有很好地平衡，则可能导致磁盘之间的负载不平衡。

RAID在平衡磁盘之间的负载方面可能做得更好（尽管似乎并不总是如此），因为它可以在较低级别平衡负载。 RAID的主要缺点是它通常会对写入吞吐量造成很大的性能影响并减少可用磁盘空间。

RAID的另一个潜在好处是能够容忍磁盘故障。但是我们的经验是重建RAID阵列是I / O密集型的，它有效地禁用了服务器，因此这并没有提供太多真正的可用性改进。

##### 应用程序 vs 系统刷新管理
**Kafka总是立即将所有数据写入文件系统，并支持配置刷新策略的功能，该策略控制何时将数据强制退出OS缓存并使用刷新进入磁盘**。 可以控制此刷新策略以在一段时间之后或在写入一定数量的消息之后将数据强制到磁盘。 此配置有多种选择。

Kafka最终必须调用fsync才能知道数据被刷新了。 当从崩溃中恢复任何未知为fsync的日志段时，Kafka将通过检查其CRC来检查每条消息的完整性，并在重新启动时执行的恢复过程中重建附带的偏移索引文件。

请注意，Kafka中的持久性不需要将数据同步到磁盘，因为失败的节点将始终从其副本中恢复。

我们建议使用默认的刷新设置，完全禁用应用程序fsync。 这意味着依赖于OS和Kafka自己的后台刷新完成的后台刷新。 这为大多数用户提供了最好的世界：无需调节旋钮，出色的吞吐量和延迟，以及完全恢复保证。 我们通常认为复制提供的保证比同步到本地磁盘更强，但偏执者仍然可能更喜欢同时支持应用程序级别的fsync策略。

使用应用程序级别刷新设置的缺点是它的磁盘使用模式效率较低（它为操作系统提供了较少的重新排序写入的余地），并且它可能会引入延迟，因为fsync在大多数Linux文件系统中阻止写入文件，而 后台刷新执行更精细的页面级锁定。

##### 了解Linux OS Flush Behavior
在Linux中，写入文件系统的数据在[pagecache](https://en.wikipedia.org/wiki/Page_cache)中维护，直到它必须写入磁盘（由于应用程序级别的fsync或操作系统自己的刷新策略）。 刷新数据是由一组称为pdflush的后台线程完成的（或者在后2.6.32内核中使用“flusher threads”）。

**Pdflush有一个可配置的策略，可以控制在缓存中维护多少脏数据以及在必须将数据写回磁盘之前的时间**。此政策在此处[描述](http://web.archive.org/web/20160518040713/http://www.westnet.com/~gsmith/content/linux-pdflush.htm)。当Pdflush无法跟上写入的数据速率时，最终会导致写入过程阻止写入中的延迟，从而减慢数据的累积。

您可以通过执行操作查看操作系统内存使用的当前状态

```
root@card-web:~# cat /proc/meminfo
MemTotal:        8174812 kB
MemFree:         1266356 kB
MemAvailable:    4335460 kB
Buffers:          316404 kB
Cached:          2881884 kB
SwapCached:            0 kB
Active:          6017256 kB
Inactive:         616060 kB
Active(anon):    3435200 kB
Inactive(anon):     3020 kB
Active(file):    2582056 kB
Inactive(file):   613040 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:               800 kB
Writeback:             0 kB
AnonPages:       3435192 kB
Mapped:           101504 kB
...
DirectMap4k:       89984 kB
DirectMap2M:     5152768 kB
DirectMap1G:     5242880 kB
```
这些值的含义在上面的链接中描述。

**使用pagecache有一些优于进程内缓存的优点**，用于存储将写入磁盘的数据：
- **I/O调度程序将连续的小写入批处理为更大的物理写入，从而提高吞吐量**。
- **I/O调度程序将尝试重新排序写入以最小化磁头移动，从而提高吞吐量**。
- 它会自动使用机器上的所有可用内存

##### 文件系统选择
Kafka在磁盘上使用常规文件，因此它对特定文件系统没有硬依赖性。 但是，使用最多的两个文件系统是EXT4和XFS。 从历史上看，EXT4有更多的用途，但最近对XFS文件系统的改进表明它具有更好的Kafka工作负载性能特性，而且不会影响稳定性。

###### 一般文件系统注意
对于用于数据目录的任何文件系统，在Linux系统上，建议在安装时使用以下选项：
- noatime：**此选项禁用在读取文件时更新文件的atime（上次访问时间）属性**。 **这可以消除大量的文件系统写入，尤其是在引导消费者的情况下**。 Kafka根本不依赖于atime属性，所以禁用它是安全的。

###### XFS 注意
XFS文件系统具有大量自动调整功能，因此无需在文件系统创建时或安装时对默认设置进行任何更改。 唯一值得考虑的调整参数是：
- largeio：这会影响stat调用报告的首选I / O大小。 虽然这可以在更大的磁盘写入上实现更高的性能，但实际上它对性能的影响很小或没有影响。
- nobarrier：对于具有电池备份缓存的底层设备，此选项可通过禁用定期写入刷新来提供更高的性能。 但是，如果底层设备运行良好，它将向文件系统报告它不需要刷新，并且此选项将不起作用。

###### EXT4 注意
TODO

#### 监控（Monitoring）
**Kafka使用Yammer Metrics进行服务器中的度量报告。 Java客户端使用Kafka Metrics，这是一种内置的度量标准注册表，可最大限度地减少传入客户端应用程序的传递依赖性。** 两者**都通过JMX公开指标，并且可以配置为使用可插入的统计报告来报告统计数据以连接到您的监控系统。**
所有Kafka费率指标都有相应的累积计数指标，后缀为-total。 例如，records-consume-rate具有名为records-consume-total的对应度量。

**查看可用指标的最简单方法是启动jconsole并将其指向正在运行的kafka客户端或服务器; 这将允许使用JMX浏览所有指标。**

##### 使用JMX进行远程监视的安全注意事项
**Apache Kafka默认禁用远程JMX**。通过为使用CLI启动的进程设置环境变量JMX_PORT或使用标准Java系统属性以编程方式启用远程JMX，可以使用JMX启用远程监视。 在生产方案中启用远程JMX时，必须启用安全性，以确保未经授权的用户无法监视或控制代理或应用程序以及运行这些代理或应用程序的平台。 请注意，默认情况下，在Kafka中对JMX禁用身份验证，并且必须通过为使用CLI启动的进程设置环境变量KAFKA_JMX_OPTS或通过设置适当的Java系统属性来覆盖生产部署的安全配置。 有关保护JMX的详细信息，请参阅使用JMX技术进行监视和管理。

实操：https://kafka.apache.org/documentation/#remote_jmx

### Zookeeper
#### 部署注意事项
Zookeeper部署保持稳健的注意事项：
- **物理/硬件/网络布局中的冗余**：尽量不要将它们全部放在同一个机架中，尝试保持冗余电源和网络路径等。典型的ZooKeeper集合有5个或者7台服务器，分别容错2台和3台服务器。如果您的部署较少，则可以使用3台服务器，但请记住，在这种情况下，您只能容忍1台服务器宕机。
- **I/O隔离**：如果您执行大量写入类型的流量，您几乎肯定希望**将事务日志放在专用磁盘组上**。对事务日志的写入是同步的（但是为了性能而进行批处理），因此并发写入会显着影响性能。 **ZooKeeper快照可以是一个这样的并发写入源，理想情况下应该写在与事务日志分开的磁盘组上。快照以异步方式写入磁盘，因此通常可以与操作系统和消息日志文件共享。您可以将服务器配置为使用带有dataLogDir参数的单独磁盘组。**
- **应用程序隔离**：除非您真正了解要在同一个服务器中安装的其他应用程序的应用程序模式，否则最好独立运行ZooKeeper（尽管这可以是与硬件功能的平衡行为）。
- **谨慎使用虚拟化**：它可以工作，具体取决于您的集群布局和读/写模式以及SLA，但虚拟化层引入的微小开销可以加起来并抛弃ZooKeeper，因为它可能对时间非常敏感。
- ZooKeeper配置：**它是用java开发，运行在JVM上，确保你给它'足够'的堆空间（我们通常用3-5G运行它们，但这主要是由于我们这里的数据集大小）**。不幸的是，我们没有一个好的公式，但请记住，**允许更多ZooKeeper状态意味着快照可能变大，大型快照会影响恢复时间。实际上，如果快照变得太大（几千兆字节），那么您可能需要增加initLimit参数，以便为服务器提供足够的时间来恢复和加入整体。**
- **监控**：JMX和4个字母单词（4lw）命令都非常有用，它们在某些情况下确实重叠（在这些情况下我们更喜欢4个字母命令，它们看起来更可预测，或者至少，它们更好地与LI监控基础设施）
- **不要过度构建集群**：大型集群，特别是在大量使用模式中，**意味着很多集群内通信（写入和后续集群成员更新的仲裁）** 。拥有更多服务器会增加您的读取容量。

总的来说，我们试图保持ZooKeeper系统尽可能小，以处理负载（加上标准的增长容量规划），并尽可能简单。 
### 协议
#### 基础概念
##### 网络
Kafka使用基于TCP上的二进制协议。该协议将所有API定义为请求响应消息对。所有消息都以大小分隔，并由以下基元类型组成。

**客户端启动一个socket连接，然后写入一系列请求消息并读回相应的响应消息。连接或断开时无需握手**。如果你保持用于许多请求的持久连接来分摊TCP握手的成本，TCP会更乐意，但是除了这种惩罚之外，连接相当轻量。

客户端可能需要维护与多个broker的连接，因为数据已分区，客户端需要与具有其数据的服务器通信。但是，通常不需要从单个客户端实例维护与单个broker的多个连接（即连接池）。

**服务器保证在单个TCP连接上，将按照发送顺序处理请求，并且响应也将按该顺序返回**。broker的对于每个连接，逐个处理其正在进行的请求，以保证这种排序。请注意，客户端可以（并且理想情况下）使用非阻塞IO来实现请求流水线操作并实现更高的吞吐量。即，**即使在等待先前请求的响应时，客户端也可以发送请求，因为未完成的请求将被缓冲在底层OS套接字缓冲区中**。所有请求都由客户端发起，并从服务器产生相应的响应消息，除非另有说明。

服务器对请求大小具有可配置的最大限制，任何超出此限制的请求都将导致套接字断开连接。

##### 分区和引导
Kafka是一个分区系统，因此并非所有服务器都具有完整的数据集。Topic被分成预定数量的分区P，并且每个分区都复制了一些复制因子N。Topic分区本质只是命令编号为0,1，...，P的“提交日志”。

**所有这种性质的系统都存在如何将特定数据分配给特定分区的问题。Kafka客户端直接控制此分配，broker自己不强制执行哪些消息应发布到特定分区的特定语义。相反，要发布消息，客户端直接将消息发送到特定分区，并在获取消息时从特定分区中获取。如果两个客户端想要使用相同的分区方案，则必须使用相同的方法来计算key到分区的映射**。

这些发布或消费数据的请求必须发送到当前充当给定分区的领导者的broker。这种情况由broker强制执行，因此对错误代理的特定分区的请求将导致NotLeaderForPartition错误代码（如下所述）。

**客户端如何找出存在哪些Topic，它们具有哪些分区以及哪些broker托管这些分区，以便它可以将其请求定位到正确的主机？**

此信息是动态的，因此您不能仅使用某个静态映射文件配置每个客户端。相反，**所有Kafka broker都可以回答描述集群当前状态的元数据请求：Topic有哪些，这些Topic具有哪些分区，哪些broker是这些分区的领导者，以及这些broker的主机和端口信息。**

换句话说，客户端需要以某种方式找到一个broker，该broker将告诉客户端存在的所有其他broker以及它们托管的分区。第一个broker本身可能会关闭，因此客户端实现的最佳实践是从两个或三个broker的URL列表进行引导。然后，用户可以选择使用负载均衡器，或者只是静态配置客户端中的两个或三个Kafka主机。

**客户端不需要继续轮询以查看群集是否已更改;它可以在实例化缓存元数据时获取元数据，直到它收到指示元数据已过期的错误**。此错误有两种形式：
1. 套接字错误，指示客户端无法与特定broker通信;
2. 响应请求中的错误代码，指示此broker不再承载请求数据的分区。

基本步骤：
- 循环查看“bootstrap”Kafka URL列表，直到找到我们可以连接的URL。获取群集元数据。
- 根据获取的元数据，从中获取主题/分区将它们定位到相应的broker。
- 如果我们收到相应的错误，请刷新元数据并重试。

##### 分区策略
在Kafka中，分区设计有两个目的：
1. 均衡了broker的数据和请求负载；
2. 作为消费者消费的分配单位，同时允许本地状态并保留分区内的偏移顺序。我们称之为**语义分区**。

为了实现简单的负载平衡，一个简单的方法是客户端对所有broker进行循环请求。另一种选择，在producers多于brokers的环境中，将让每个客户随机选择一个分区并发布到该分区。后一种策略将导致更少的TCP连接。

**语义分区意味着使用消息中的某个键将消息分配给分区（如哈希取模）**。例如，如果您正在处理单击消息流，则可能需要按用户标识对流进行分区，以便特定用户的所有数据都将转到单个消费者。 为此，客户端可以获取消息中用户标识关联的key，并使用此key的一些散列来选择要将消息传递到的分区。

##### 批处理
**我们的API鼓励将小东西（小I/O请求等）拼凑在一起以提高效率。** 我们发现这是非常重要的策略。我们用于发送消息的API和用于获取消息的API始终使用一系列消息而不是单个消息来描述这一点。 聪明的客户端可以利用它并支持“异步”模式，**在该模式下，它将单独发送的消息批处理并以更大的块形式发送**。我们更进一步，**并允许批处理多个主题和分区，因此产生请求可能包含附加到许多分区的数据，并且获取请求可以同时从多个分区中提取数据。**

客户端实现者可以选择忽略它，如果愿意，可以一次发送一个。

##### 版本控制和兼容性
该协议旨在以向后兼容的方式实现增量演进。我们的版本控制基于每个API，每个版本包含一个请求和响应对。每个请求都包含一个API密钥，用于标识要调用的API以及一个版本号，用于指示请求的格式和预期的响应格式。

目的是客户端将支持一系列API版本。与特定broker进行通信时，给定客户端应使用两者支持的最高API版本，并在其请求中指明此版本。

## API
## SourceCode
## 调优
### 性能
> 性能报告源自：https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines

## FAQ