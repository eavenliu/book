# 高性能的数据传输-提升TCP性能

# 概述
这些说明旨在帮助用户和系统管理员最大化其计算机系统上的TCP/IP性能。他们总结了所有终端系统（计算机系统）网络调优问题，包括TCP调优。

# 介绍
我们在针对应用的访问情况分析的时候，网络性能调试（通常委婉地称为“TCP调优”）非常困难，因为几乎所有缺陷都具有完全相同的症状：性能降低。例如，TCP缓冲区空间不足以与过多的数据包丢失无法区分（通过TCP重新传输进行无提示修复），因为这两个缺陷只会减慢应用程序的速度，而不会出现任何特定的识别症状。

**缺陷分为三大类：应用程序本身，计算机系统（包括操作系统和TCP调优）和网络路径**。这些领域中的每一个都需要一种非常不同的性能调试方法。此页面的重点是帮助用户和系统管理员优化其计算机系统上的TCP/IP。
- 应用程序。应用程序本身需要针对不同的业务情况进行对应的代码编写和调试。
- 网络路径。可能很难调试，因为TCP补偿缺陷的能力与往返时间（RTT）成反比。因此，例如，导致应用程序在1毫秒路径上花费额外秒数的缺陷通常会导致相同的应用程序在10毫秒路径上花费额外的10秒。这种“症状缩放”效应的产生是因为TCP补偿缺陷的能力是在往返中计量的：如果给定的缺陷在50次往返中得到补偿（典型的是中速链路上的损失），那么单次丢失会影响1 ms的路径仅50毫秒，500毫秒的10毫秒路径。症状缩放使得诊断特别困难，因为  在长路径上完整显示阻塞的缺陷通常在短路径上是不可检测的。

目标是总结所有终端系统网络调优问题，为非专家提供简单的配置检查，并维护操作系统特定建议的存储库以及有关在这些平台上获得最佳网络性能的信息。

请注意，今天大多数TCP实现都非常好。主要缺陷是默认配置，适用于局域网（LAN）和互联网后路：数百万相对低速的家庭用户。

# 基础概念
今天互联网上使用的主要协议是TCP，**一种“可靠的”“基于窗口的”协议**。当发送方和接收方之间的网络管道保持充满数据时，可以实现最佳的网络性能。

## BDP
**可以在网络中传输的数据量（称为“Bandwidth-Delay-Product”或简称BDP，带宽-延迟-产品）仅仅是瓶颈链路带宽和往返时间（RTT）的乘积**。

**BDP是基于窗口的协议（如TCP）中的一个简单但重要的概念**。下面讨论的一些问题是因为当今网络的BDP已经超出了最初设计TCP/IP协议时的BDP。为了适应BDP的大幅增加，已经在TCP协议中提出并实现了一些高性能扩展。但是，默认情况下有时不会启用这些高性能选项，系统管理员必须明确启用这些选项。

## 缓冲区
在诸如TCP的“可靠”协议中，上述BDP的重要性在于这是终端主机（发送方和接收方）中所需的缓冲量。原始TCP（没有高性能选项）支持的最大缓冲区限制为64K字节。如果BDP很小，或者因为链路很慢或者因为RTT很小（例如在LAN中），则默认配置通常是足够的。但是对于具有大BDP的路径，因此需要大缓冲区，必须启用下一节中讨论的高性能选项。

## 计算BDP
要计算BDP，我们需要知道路径中**最慢链路的速度**和**往返时间（RTT）**。

链路的峰值带宽通常以Mbit/s（或最近以Gbit/s为单位）表示。**广域链路的往返延迟（RTT）通常在1毫秒到100毫秒之间，可通过ping或traceroute测量**。 

例如，对于两台带有GigE卡的主机，通过Abilene跨越Coast-to-coast链路进行通信，瓶颈链路将是GigE卡本身。实际往返时间（RTT）可以使用ping测量，但在本例中我们将使用70毫秒。

了解瓶颈链接速度和RTT后，BDP可按如下方式计算：

```
1,000,000,000位 * 1个字节（8位） * 0.07秒= 8,750,000字节= 8.75 MB

```
基于这些计算，很容易理解为什么64KB的典型默认缓冲区大小完全不适合此连接。使用64KB时，您只能获得0.1％的可用带宽。

# 高性能网络选项
以下选项按照应检查和调整的顺序逐个分析
## 1、最大TCP缓冲区（内存）空间
**所有操作系统都有一些全局机制来限制任何一个TCP连接可以使用的系统内存量。**

不同的系统，限制会有不同：
- 在某些系统上，每个连接都受内存限制的约束，该内存限制应用于用于输入数据，输出数据和控制结构的总内存；
- 在其他系统上，每个连接的输入和输出缓冲区空间都有单独的限制。

具体的调整会在后面涉及。

## 2、套接字缓冲区大小
**大多数操作系统还支持单独的每个连接发送和接收缓冲区限制，只要它们保持在上述最大内存限制范围内，就可以由用户，应用程序或其他机制进行调整**。这些缓冲区大小对应于BSD setsockopt（）调用的SO_SNDBUF和SO_RCVBUF选项。

**套接字缓冲区必须足够大，以容纳TCP数据的完整BDP加上一些特定于操作系统的开销**。它们还确定接收器窗口（rwnd），用于实现TCP连接两端之间的流量控制。有几种方法可用于调整套接字缓冲区大小：
- **TCP自动调整** ：会根据需要自动调整套接字缓冲区大小，以最佳地平衡TCP性能和内存使用情况。 现在，在当前Linux版本中（2.6.6和2.4.16之后）默认启用自动调整。
- **缺省套接字缓冲区大小** ：一般可以用全局控制设置。**这些默认大小用于未以其他方式设置的所有套接字缓冲区大小**。对于单用户系统，手动调整默认缓冲区大小是调整任意应用程序的最简单方法。同样，没有标准方法来执行此操作，参考后面的配置流程。
- **由于过度缓冲可能导致某些应用程序行为不佳（通常导致缓慢的交互式响应）以及使系统内存不足的风险，因此必须在多用户系统上仔细考虑较大默认套接字缓冲区**。我们通常建议使用略大于64 kBytes的默认套接字缓冲区大小，这对于大多数环境中的最佳批量传输性能而言仍然太小。它具有缓解调试TCP窗口缩放选项（见下文）的一些困难的优点，而不会由于过度缓冲交互式应用程序而导致问题。
- **对于应用程序，工程师可以通过setsockopt() 这个系统调用方法来调整套接字缓冲区大小。**
- **一些常见的应用程序包含了内置开关或命令，允许用户手动设置套接字缓冲区大小**。最常见的示例包括iperf（网络诊断），许多ftp变体（包括gridftp）和其他批量数据复制工具。
- 其他方式暂不分析

## 3、TCP大型窗口扩展（RFC1323）
它们支持可选的TCP协议功能（窗口缩放和时间戳），这些功能是支持大型BDP路径所必需的。

**窗口比例选项（WSCALE）**是最重要的RFC1323功能，并且可能非常难以正确使用。窗口比例提供了一个比例因子，这是TCP支持大于64k字节的窗口大小所需的。大多数系统在某些情况下会自动请求WSCALE，例如当接收套接字缓冲区大于64k字节或TCP连接的另一端首先请求它时。**WSCALE只能在连接开始时进行协商。如果任一端未能请求WSCALE或请求不足的值，则在同一连接期间无法稍后重新协商**。虽然不同的系统使用不同的算法来选择WSCALE，但它们通常都是最大允许缓冲区大小，此连接的当前接收器缓冲区大小，或者在某些情况下是全局系统设置的函数。

请注意，在这些约束下（许多平台都很常见），**希望以高速率发送数据的客户端应用程序可能需要在打开连接之前将其自己的接收缓冲区设置为大于64k字节的内容，以确保服务器正确协商WSCALE**。

一些系统要求系统管理员明确启用RFC1323扩展。如果系统不能（或不）协商WSCALE，则它不支持大于64k字节的TCP窗口大小（BDP）。

另一个RFC1323功能是**TCP时间戳选项**，它可以更好地测量往返时间，并保护TCP免受数据损坏的影响，如果数据包的传送时间太长，以至于序列号在传送之前就会换行。包裹的序列号不会造成低于100 Mb / s的严重风险，但随着数据速率的提高，风险会逐渐变大。

由于改进的RTT估计，许多系统使用时间戳甚至低速率。

## 4、TCP选择性确认选项（SACK，RFC2018）
允许TCP接收器准确地通知发送方哪些数据丢失并需要重新传输。

如果没有SACK，TCP必须估计哪些数据丢失，如果所有丢失都被隔离（在任何给定的往返中只丢失一次）这就做得还好。如果没有SACK，TCP通常需要很长时间才能在丢失集群后进行恢复，这对于即使是轻微拥塞的大型BDP路径也是正常情况。大多数操作系统现在都支持SACK，但可能必须由系统管理员明确打开。

如果你有一个不支持SACK的系统，你通常可以通过略微使其无法容纳套接字缓冲区空间来提高TCP性能。缓冲区饥饿可以防止TCP将路径驱动到拥塞状态，并最大限度地减少导致集群丢失的可能性。

## 5、路径MTU
**主机系统必须使用路径的最大可能MTU**。这可能需要启用Path MTU Discovery RFC1191，RFC1981，RFC4821）。

# 使用基于Web的网络诊断服务器
通过来自适当诊断服务器的单个测试，可以诊断大多数调整问题（以及许多其他网络问题）。有几种不同的服务器可以测试终端系统和网络路径的各个方面。

fiddler wireshark

# 在各种操作系统下进行系统调整的详细过程

## 在FreeBSD下提高网络限制的程序
可以使用“sysctl”读取或设置所有系统参数。例如：

```
sysctl [parameter] 
sysctl -w [parameter] = [value]
```

您可以通过以下方式提高最大套接字缓冲区大小：

```
sysctl -w kern.ipc.maxsockbuf = 4000000
```
FreeBSD 7.0实现了默认启用的自动接收和发送缓冲区调整。默认最大值为256KB，可能太小。这些应该增加，例如如下：

```
net.inet.tcp.sendbuf_max = 16777216 
net.inet.tcp.recvbuf_max = 16777216
```
您还可以使用变量设置TCP和UDP默认缓冲区大小

```
net.inet.tcp.sendspace 
net.inet.tcp.recvspace 
net.inet.udp.recvspace
```
使用较大的套接字缓冲区时，可能需要确保启用了TCP窗口缩放选项。（默认未启用！）检查/etc/rc.conf中的'tcp_extensions =“YES”'并确保通过sysctl变量启用它：

```
net.inet.tcp.rfc1323
```
FreeBSD的TCP默认启用了一个名为“inflight restriction”的东西，在某些情况下这可能对TCP吞吐量有害。如果你想要“正常”的TCP行为，你应该这样做

```
sysctl -w net.inet.tcp.inflight_enable = 0
```
您可能还想确认已启用SACK :(自FreeBSD 5.3起工作）：

```
net.inet.tcp.sack.enable
```
默认情况下，在FreeBSD中启用MTU发现。如果要禁用MTU发现，可以使用sysctl变量进行切换：

```
net.inet.tcp.path_mtu_discovery
```
## 调整Linux 2.4和2.6的TCP
注意：最新版本的Linux（版本2.6.17及更高版本）具有完全自动调整功能，最大缓冲区大小为4MB。除了在极少数情况下，手动调优不太可能在大多数网络路径上显着提高这些内核的性能，通常不建议

由于自动调整和大型默认缓冲区大小在一系列不同的内核版本上逐步发布，因此最好检查并仅根据需要调整调整。升级内核时，您可能需要考虑删除任何本地调整。

可以通过访问/ proc文件系统中的特殊文件来读取或设置所有系统参数。例如：
```
root@card-web:~# cat /proc/sys/net/ipv4/tcp_moderate_rcvbuf
1
```
**如果参数tcp_moderate_rcvbuf存在且值为1，则自动调整有效**。通过自动调整，可以为每个连接动态更新（自动调整）接收器缓冲区大小（和TCP窗口大小）。（发送方自动调整已存在且无条件启用多年）。

每个连接内存空间默认值设置为两个3元素数组：
```
#/proc/sys/net/ipv4/tcp_rmem  - 为TCP rcv缓冲区保留的内存
#/proc/sys/net/ipv4/tcp_wmem  - 为TCP snd缓冲区保留的内存

root@card-web:~# cat /proc/sys/net/ipv4/tcp_rmem
10240	87380	12582912
root@card-web:~# cat /proc/sys/net/ipv4/tcp_wmem
10240	87380	12582912
```
**这些是三个值的数组：最小，初始和最大缓冲区大小**。它们用于设置自动调整的界限，并在内存压力下平衡内存使用。请注意，这些是对实际内存使用量的控制（不仅仅是TCP窗口大小），包括套接字数据结构使用的内存以及大缓冲区中短数据包浪费的内存。通过一些合适的开销，最大值必须大于路径的BDP。

**使用自动调整，中间值仅确定初始缓冲区大小。对于典型的小流量，最好将其设置为某个最佳值。** 使用自动调整，过大的初始缓冲区浪费内存甚至会损害性能。

如果不存在自动调整（2.4.2之前的Linux 2.4或2.6.7之前的Linux 2.6），您可能希望获得更新的内核。或者，您可以通过将中间tcp_rmem值设置为计算的BDP来调整所有TCP连接的默认套接字缓冲区大小。 **对于具有自动调整功能的内核，建议不要这样做。 由于发送方是自动调整的，因此tcp_wmem永远不推荐这样做。**

应用程序可以请求的最大缓冲区大小（对于setsockopt（）系统调用的SO_SNDBUF和SO_RCVBUF参数的最大可接受值）可以使用/proc变量进行限制：
```
#/proc/sys/net/core/rmem_max  - 最大接收窗口
#/proc/sys/net/core/wmem_max  - 最大发送窗口

root@card-web:~# cat /proc/sys/net/core/rmem_max
12582912
root@card-web:~# cat /proc/sys/net/core/wmem_max
12582912
```
内核将实际内存限制设置为请求值的两倍（有效地使rmem_max和wmem_max加倍）以提供足够的内存开销。除非您计划使用某种形式的应用程序调整，否则您无需调整这些。

> 注意：使用setsockopt（）手动调整套接字缓冲区大小会禁用自动调整。针对其他操作系统优化的应用程序可能会隐含地破坏Linux自动调整。

以下值（对于具有超过1 GB内存的2.6.17的默认值）对于具有4MB BDP或更小的所有路径（您必须是root）是合理的：

```
echo 1 > /proc/sys/net/ipv4/tcp_moderate_rcvbuf
echo 108544 > /proc/sys/net/core/wmem_max
echo 108544 > /proc/sys/net/core/rmem_max
echo “4096 87380 4194304” > /proc/sys/net/ipv4/tcp_rmem
echo “4096 16384 4194304” > /proc/sys/net/ipv4/tcp_wmem
```
除非您确切知道自己在做什么，否则不要调整tcp_mem。此数组（以页为单位）确定系统如何平衡总网络缓冲区空间与所有其他LOWMEM内存使用情况。这三个元素在引导时初始化为可用系统内存的适当部分。

您无需调整rmem_default或wmem_default（至少不适用于TCP调整）。这些是非TCP套接字的默认缓冲区大小（例如，unix域和UDP套接字）。

默认情况下，所有标准高级TCP功能都处于启用，你可以通过以下方式检查：

```
oot@card-web:~# cat /proc/sys/net/ipv4/tcp_timestamps
1
root@card-web:~# cat /proc/sys/net/ipv4/tcp_window_scaling
1
root@card-web:~# cat /proc/sys/net/ipv4/tcp_sack
1
```

**Linux支持/proc和sysctl（使用变量名的替代形式 - 例如net.core.rmem_max）来检查和调整网络调整参数。以下是检查所有tcp参数的有用快捷方式：sysctl -a | fgrep tcp **

```
root@card-web:~# sysctl -a | fgrep tcp
net.ipv4.tcp_abort_on_overflow = 0
net.ipv4.tcp_adv_win_scale = 1
net.ipv4.tcp_allowed_congestion_control = cubic reno
net.ipv4.tcp_app_win = 31
net.ipv4.tcp_autocorking = 1
net.ipv4.tcp_available_congestion_control = cubic reno
net.ipv4.tcp_base_mss = 1024
net.ipv4.tcp_challenge_ack_limit = 1000
net.ipv4.tcp_congestion_control = cubic
net.ipv4.tcp_dsack = 1
net.ipv4.tcp_early_retrans = 3
net.ipv4.tcp_ecn = 2
net.ipv4.tcp_ecn_fallback = 1
net.ipv4.tcp_fack = 1
net.ipv4.tcp_fastopen = 1
net.ipv4.tcp_fastopen_key = 00000000-00000000-00000000-00000000
net.ipv4.tcp_fin_timeout = 30
net.ipv4.tcp_frto = 2
......
```
有关内核变量的其他信息，请查看内核源代码中包含的文档，通常位于某些位置，例如  /usr/src/linux-<version>/Documentation/networking/ip-sysctl.txt。网络sysctl上有一个非常好的（但稍微过时）的教程 https://www.frozentux.net/ipsysctl-tutorial/ipsysctl-tutorial.html